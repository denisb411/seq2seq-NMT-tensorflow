{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The helper file\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "CODES = {'<unk>': 0, '<s>': 1, '</s>': 2}\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_and_save_data(source_path, target_path):\n",
    "    \"\"\"\n",
    "    Preprocess Text Data.  Save to to file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess\n",
    "    source_text = load_data(source_path)\n",
    "    target_text = load_data(target_path)\n",
    "\n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "\n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "    \n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "    \n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "    # Save Data\n",
    "    pickle.dump((\n",
    "        (source_text, target_text),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))\n",
    "\n",
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    \"\"\"\n",
    "    vocab = set(text.split())\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "    \n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))\n",
    "\n",
    "def batch_data(source, target, batch_size):\n",
    "    \"\"\"\n",
    "    Batch source and target together\n",
    "    \"\"\"\n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = source[start_i:start_i + batch_size]\n",
    "        target_batch = target[start_i:start_i + batch_size]\n",
    "        yield np.array(pad_sentence_batch(source_batch)), np.array(pad_sentence_batch(target_batch))\n",
    "\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"\n",
    "    Pad sentence with </s> id\n",
    "    \"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [CODES['</s>']] * (max_sentence - len(sentence))\n",
    "            for sentence in sentence_batch]\n",
    "\n",
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    source_text_to_id = [[source_vocab_to_int[word] for word in line.split()] for line in source_text.split('\\n')]\n",
    "    target_text_to_id = [[target_vocab_to_int[word] for word in line.split()] for line in target_text.split('\\n')]\n",
    "    \n",
    "    return (source_text_to_id, target_text_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m           \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudart_dll_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1c11152533f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msource_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data/small_vocab_fr'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtarget_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data/small_vocab_en'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msource_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Perform pre-load sanity checks in order to produce a more actionable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# than we get from an error during SWIG import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mself_check\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m               \u001b[1;34m\"environment variable. Download and install CUDA %s from \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m               \u001b[1;34m\"this URL: https://developer.nvidia.com/cuda-toolkit\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m               % (build_info.cudart_dll_name, build_info.cuda_version_number))\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m       if hasattr(build_info, \"cudnn_dll_name\") and hasattr(\n",
      "\u001b[1;31mImportError\u001b[0m: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "source_path = 'data/small_vocab_fr'\n",
    "target_path = 'data/small_vocab_en'\n",
    "source_text = load_data(source_path)\n",
    "target_text = load_data(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8c838d8eb993>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreprocess_and_save_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'source_path' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess_and_save_data(source_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "assert LooseVersion(tf.__version__) in [LooseVersion('1.5.0'), LooseVersion('1.5.1')], 'This project requires TensorFlow version 1.5  You are using {}'.format(tf.__version__)\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "# pad_sentence_batch(source_int_text)\n",
    "source_vocab = len(source_vocab_to_int)\n",
    "target_vocab = len(target_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 230)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Seq2seqHyperparams(object):\n",
    "    def __init__(self, hidden_units=256, n_layers_enconder=2,\n",
    "                 n_layers_decoder=2, num_encoder_symbols=source_vocab, \n",
    "                 num_decoder_symbols=target_vocab, learning_rate=0.01,\n",
    "                 embedding_size=15, max_gradient_norm=5.0, dtype=tf.float32,\n",
    "                 epochs=1, batch_size_train=256, dropout=0.2, forget_bias=1.0,\n",
    "                 use_beam_search=True, beam_width=10, length_penalty_weight=0.0,\n",
    "                 use_attention=True, learning_rate_decay=False, \n",
    "                 use_bidirectional_enconder=False)\n",
    "    \n",
    "        self.hidden_units = hidden_units\n",
    "        self.n_layers_enconder = n_layers_enconder\n",
    "        self.n_layers_decoder = n_layers_decoder\n",
    "        self.num_encoder_symbols = num_encoder_symbols\n",
    "        self.num_decoder_symbols = num_decoder_symbols\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.dtype = dtype\n",
    "        self.epochs = epochs\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.dropout = dropout\n",
    "        self.forget_bias = forget_bias\n",
    "        self.use_beam_search = use_beam_search\n",
    "        self.beam_width = beam_width\n",
    "        self.length_penalty_weight = length_penalty_weight\n",
    "        self.use_attention = use_attention\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.use_bidirectional_enconder = use_bidirectional_enconder\n",
    "\n",
    "\n",
    "        # Extra vocabulary symbols\n",
    "        unk = '<unk>'\n",
    "        sos = '<s>'\n",
    "        eos = '</s>' # also function as PAD\n",
    "        self.extra_tokens = [unk, sos, eos]\n",
    "        self.unk_token = extra_tokens.index(unk) #unk_token = 0\n",
    "        self.start_token = extra_tokens.index(sos) # start_token = 1\n",
    "        self.end_token = extra_tokens.index(eos)   # end_token = 2\n",
    "\n",
    "hparams = Seq2seqHyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.contrib.rnn import MultiRNNCell\n",
    "from tensorflow import layers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    ### DEFINING PLACEHOLDERS ###\n",
    "\n",
    "    # encoder_inputs: [batch_size, max_time_steps]\n",
    "    encoder_inputs = tf.placeholder(dtype=tf.int32,\n",
    "                shape=(None, None), name='encoder_inputs')\n",
    "\n",
    "    # encoder_inputs_length: [batch_size]\n",
    "    encoder_inputs_length = tf.placeholder(\n",
    "                dtype=tf.int32, shape=(None,), name='encoder_inputs_length')\n",
    "\n",
    "    # get dynamic batch_size\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "\n",
    "    ### TRAIN MODE PLACEHOLDERS ###\n",
    "\n",
    "    # decoder_inputs: [batch_size, max_time_steps]\n",
    "    decoder_inputs = tf.placeholder(\n",
    "                    dtype=tf.int32, shape=(None, None), name='decoder_inputs')\n",
    "\n",
    "    # decoder_inputs_length: [batch_size]\n",
    "    decoder_inputs_length = tf.placeholder(\n",
    "                    dtype=tf.int32, shape=(None,), name='decoder_inputs_length')\n",
    "\n",
    "    decoder_start_token = tf.ones(\n",
    "                    shape=[batch_size, 1], dtype=tf.int32) * hparams.start_token\n",
    "    decoder_end_token = tf.ones(\n",
    "                    shape=[batch_size, 1], dtype=tf.int32) * hparams.end_token  \n",
    "\n",
    "\n",
    "    # decoder_inputs_train: [batch_size , max_time_steps + 1]\n",
    "    # insert sos symbol in front of each decoder input\n",
    "    decoder_inputs_train = tf.concat([decoder_start_token,\n",
    "                                          decoder_inputs], axis=1)\n",
    "\n",
    "    # decoder_inputs_length_train: [batch_size]\n",
    "    decoder_inputs_length_train = decoder_inputs_length + 1\n",
    "\n",
    "    # decoder_targets_train: [batch_size, max_time_steps + 1]\n",
    "    # insert eos symbol at the end of each decoder input\n",
    "    decoder_targets_train = tf.concat([decoder_inputs,\n",
    "                                           decoder_end_token], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## DEFINING ENCODER ##\n",
    "\n",
    "    encoder_embeddings = tf.Variable(tf.random_uniform([hparams.num_encoder_symbols, hparams.embedding_size], -1.0, 1.0),\n",
    "                                     dtype=hparams.dtype)\n",
    "\n",
    "    # Embedded_inputs: [batch_size, time_step, embedding_size]\n",
    "    encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "        params=encoder_embeddings, ids=encoder_inputs)\n",
    "\n",
    "    if use_bidirectional_enconder:\n",
    "        \n",
    "        num_bi_layers = int(hparams.n_layers_enconder / 2)\n",
    "        num_residual_layers = hparams.n_layers_enconder - 1\n",
    "        num_bi_residual_layers = int(num_residual_layers / 2)\n",
    "        \n",
    "        print(num_bi_layers, num_residual_layers, num_bi_residual_layers)\n",
    "        \n",
    "        cell_list = []\n",
    "        for i in range(hparams.n_layers_enconder):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(hparams.hidden_units, forget_bias=hparams.forget_bias)\n",
    "\n",
    "            if (i >= hparams.n_layers_enconder - num_residual_layers):\n",
    "                cell = tf.contrib.rnn.ResidualWrapper(cell, residual_fn=None)\n",
    "                if hparams.dropout > 0.0:\n",
    "                    cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                        cell=cell, input_keep_prob=(1.0 - hparams.dropout))\n",
    "            \n",
    "            cell_list.append(cell)\n",
    "            \n",
    "        if len(cell_list) == 1:  # Single layer.\n",
    "            fw_cell = cell_list[0]\n",
    "            bw_cell = cell_list[0]\n",
    "        else:  # Multi layers\n",
    "            fw_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "            bw_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "\n",
    "        fw_cell = tf.contrib.rnn.BasicLSTMCell(hparams.n_layers_enconder)\n",
    "        bw_cell = tf.contrib.rnn.BasicLSTMCell(hparams.n_layers_enconder)\n",
    "\n",
    "        bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                                        fw_cell,\n",
    "                                                        bw_cell,\n",
    "                                                        encoder_inputs_embedded,\n",
    "                                                        dtype=dtype,\n",
    "                                                        sequence_length=encoder_inputs_length,\n",
    "                                                        time_major=False,\n",
    "                                                        swap_memory=True)\n",
    "        print(bi_outputs, \"\\n\\n\", bi_state)\n",
    "\n",
    "        encoder_outputs, bi_encoder_state = tf.concat(bi_outputs, -1), bi_state\n",
    "        \n",
    "#         if num_bi_layers == 1:\n",
    "#             encoder_last_state = bi_encoder_state\n",
    "#         else:\n",
    "#             # alternatively concat forward and backward states\n",
    "#             encoder_state = []\n",
    "#             for layer_id in range(num_bi_layers):\n",
    "#                 encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "#                 encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "#             encoder_last_state = tuple(encoder_state)\n",
    "\n",
    "        encoder_state = bi_encoder_state\n",
    "        \n",
    "    else:\n",
    "        # Build RNN cell\n",
    "        cells = []\n",
    "        for _ in range(hparams.n_layers_enconder):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(hparams.hidden_units, forget_bias=hparams.forget_bias)\n",
    "            if hparams.dropout > 0.0:\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                    cell=cell, input_keep_prob=(1.0 - hparams.dropout))\n",
    "            cells.append(cell)\n",
    "        if n_layers_enconder == 1:\n",
    "            encoder_cells = cells[0]\n",
    "        else:\n",
    "            encoder_cells = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "        encoder_outputs, encoder_last_state = tf.nn.dynamic_rnn(\n",
    "            cell=encoder_cells, inputs=encoder_inputs_embedded,\n",
    "            sequence_length=encoder_inputs_length, dtype=hparams.dtype,\n",
    "            time_major=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ### DEFINING DECODER ###\n",
    "\n",
    "    # Building decoder_cell\n",
    "    cells = []\n",
    "    # Build RNN cell\n",
    "    for _ in range(hparams.n_layers_decoder):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(hparams.hidden_units, forget_bias=hparams.forget_bias)\n",
    "        if hparams.dropout > 0.0:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell=cell, input_keep_prob=(1.0 - hparams.dropout))\n",
    "        cells.append(cell)\n",
    "    if hparams.n_layers_decoder == 1:\n",
    "        decoder_cells = cells[0]\n",
    "    else:\n",
    "        decoder_cells = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "    if hparams.use_attention:\n",
    "        memory = encoder_outputs\n",
    "        \n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "            hparams.hidden_units,\n",
    "            memory,\n",
    "            memory_sequence_length=encoder_inputs_length,\n",
    "            normalize=True)\n",
    "        \n",
    "        decoder_cells_train = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            decoder_cells,\n",
    "            attention_mechanism,\n",
    "            attention_layer_size=hparams.hidden_units,\n",
    "            alignment_history=False,\n",
    "            output_attention=True,\n",
    "            name=\"attention\")\n",
    "        \n",
    "        decoder_initial_state = decoder_cells_train.zero_state(batch_size, hparams.dtype).clone(\n",
    "          cell_state=encoder_last_state)\n",
    "        \n",
    "    else:\n",
    "        decoder_cells_train = decoder_cells\n",
    "        decoder_initial_state = encoder_last_state\n",
    "\n",
    "    decoder_embeddings = tf.Variable(tf.random_uniform([hparams.num_decoder_symbols, embedding_size], -1.0, 1.0), dtype=hparams.dtype)\n",
    "    \n",
    "    # decoder_inputs_embedded: [batch_size, max_time_step + 1, embedding_size]\n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "        params=decoder_embeddings, ids=decoder_inputs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ### TRAIN MODE ###\n",
    "    \n",
    "    # Helper to feed inputs for training: read inputs from dense ground truth vectors\n",
    "    training_helper = seq2seq.TrainingHelper(inputs=decoder_inputs_embedded,\n",
    "                                       sequence_length=decoder_inputs_length_train,\n",
    "                                       time_major=False,\n",
    "                                        name='training_helper')\n",
    "\n",
    "    training_decoder = seq2seq.BasicDecoder(cell=decoder_cells_train,\n",
    "                                       helper=training_helper,\n",
    "                                       initial_state=decoder_initial_state)\n",
    "\n",
    "    # decoder_outputs_train: BasicDecoderOutput\n",
    "    #                        namedtuple(rnn_outputs, sample_id)\n",
    "    # decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, num_decoder_symbols] if output_time_major=False\n",
    "    #                                   [max_time_step + 1, batch_size, num_decoder_symbols] if output_time_major=True\n",
    "    # decoder_outputs_train.sample_id: [batch_size], tf.int32\n",
    "    (decoder_outputs_train, decoder_last_state_train, \n",
    "         decoder_outputs_length_decode)  = seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                        output_time_major=False,\n",
    "                                                        swap_memory=True,\n",
    "                                                        impute_finished=True)\n",
    "\n",
    "    # More efficient to do the projection on the batch-time-concatenated tensor\n",
    "    # logits_train: [batch_size, max_time_step + 1, num_decoder_symbols]\n",
    "    \n",
    "    sample_id = decoder_outputs_train.sample_id\n",
    "    \n",
    "    output_layer = layers.Dense(num_decoder_symbols, name='output_projection')\n",
    "    logits_train = output_layer(decoder_outputs_train.rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    \n",
    "    ### LOSS, GRADIEND AND OPTIMIZATION ###\n",
    "    \n",
    "    if hparams.learning_rate_decay:\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        learning_rate = tf.constant(hparams.learning_rate)\n",
    "\n",
    "        #using luong10 decay scheme\n",
    "        decay_factor = 0.5\n",
    "        start_decay_step = int(hparams.epochs / 2)\n",
    "        decay_times = 10\n",
    "\n",
    "        remain_steps = hparams.epochs - start_decay_step\n",
    "        decay_steps = int(remain_steps / decay_times)\n",
    "\n",
    "        learning_rate = tf.cond(global_step < start_decay_step,\n",
    "                                lambda: hparams.learning_rate,\n",
    "                                lambda: tf.train.exponential_decay(\n",
    "                                    hparams.learning_rate,\n",
    "                                    (global_step - start_decay_step),\n",
    "                                    decay_steps, decay_factor, staircase=True),\n",
    "                                name=\"learning_rate_decay_cond\")\n",
    "    \n",
    "    # Maximum decoder time_steps in current batch\n",
    "    max_decoder_length = tf.reduce_max(decoder_inputs_length_train)\n",
    "    \n",
    "    # masks: masking for valid and padded time steps, [batch_size, max_time_step + 1]\n",
    "    target_weights = tf.sequence_mask(lengths=decoder_inputs_length_train, \n",
    "                             maxlen=max_decoder_length, dtype=hparams.dtype, name='masks')\n",
    "    \n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=decoder_targets_train, logits=logits_train)\n",
    "    \n",
    "    loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "        tf.cast(batch_size, dtype=hparams.dtype))\n",
    "\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    gradients = tf.gradients(loss, \n",
    "                             trainable_params)\n",
    "    \n",
    "    clip_gradients, gradient_norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
    "    \n",
    "    updates = opt.apply_gradients(\n",
    "            zip(clip_gradients, trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "\n",
    "    ### INFERENCE MODE ###\n",
    "    \n",
    "    start_tokens = tf.fill([batch_size], hparams.start_token)\n",
    "    \n",
    "    if hparams.use_attention:\n",
    "        memory = tf.contrib.seq2seq.tile_batch(\n",
    "          memory, multiplier=hparams.beam_width)\n",
    "        \n",
    "        source_sequence_length = tf.contrib.seq2seq.tile_batch(\n",
    "          encoder_inputs_length, multiplier=hparams.beam_width)\n",
    "        \n",
    "        encoder_last_state = tf.contrib.seq2seq.tile_batch(\n",
    "          encoder_last_state, multiplier=hparams.beam_width)\n",
    "        \n",
    "        batch_size = batch_size * hparams.beam_width\n",
    "        \n",
    "        attention_mechanism_infer = tf.contrib.seq2seq.BahdanauAttention(\n",
    "            hparams.hidden_units,\n",
    "            memory,\n",
    "            memory_sequence_length=source_sequence_length,\n",
    "            normalize=True)\n",
    "        \n",
    "        decoder_cells_infer = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            decoder_cells,\n",
    "            attention_mechanism_infer,\n",
    "            attention_layer_size=hparams.hidden_units,\n",
    "            alignment_history=False,\n",
    "            output_attention=True,\n",
    "            name=\"attention_infer\")\n",
    "        \n",
    "        decoder_initial_state_infer = decoder_cells_infer.zero_state(batch_size, hparams.dtype).clone(\n",
    "          cell_state=encoder_last_state)\n",
    "    \n",
    "    if hparams.use_beam_search:\n",
    "        #     decoder_initial_state_infer = tf.contrib.seq2seq.tile_batch(\n",
    "        #           encoder_last_state, multiplier=beam_width)\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "              cell=decoder_cells_infer,\n",
    "              embedding=decoder_embeddings,\n",
    "              start_tokens=start_tokens,\n",
    "              end_token=hparams.end_token,\n",
    "              initial_state=decoder_initial_state_infer,\n",
    "              beam_width=beam_width,\n",
    "              output_layer=output_layer,\n",
    "              length_penalty_weight=length_penalty_weight)\n",
    "        \n",
    "    else:\n",
    "        inference_helper = seq2seq.GreedyEmbeddingHelper(decoder_embeddings,\n",
    "                                                        start_tokens=start_tokens,\n",
    "                                                        end_token=hparams.end_token)\n",
    "\n",
    "        inference_decoder = seq2seq.BasicDecoder(cell=decoder_cells_infer,\n",
    "                                                 helper=inference_helper,\n",
    "                                                 initial_state=decoder_initial_state,\n",
    "                                                 output_layer=output_layer)\n",
    "    \n",
    "    maximum_iterations = tf.round(tf.reduce_max(encoder_inputs_length) * 2)\n",
    "    \n",
    "    (decoder_infer_outputs, decoder_infer_last_state,\n",
    "                 decoder_infer_outputs_length) = (seq2seq.dynamic_decode(\n",
    "                    decoder=inference_decoder,\n",
    "                    output_time_major=False,\n",
    "                    maximum_iterations=maximum_iterations))\n",
    "    \n",
    "    if hparams.use_beam_search:\n",
    "        decoder_pred_decode = decoder_infer_outputs.predicted_ids\n",
    "        tf.identity(decoder_pred_decode, 'decoder_pred_decode')\n",
    "    \n",
    "    else:\n",
    "        logits_infer = decoder_infer_outputs.rnn_output\n",
    "        sample_id_infer = decoder_infer_outputs.sample_id                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21]\n",
      "[17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'updates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-01c3febc2a53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             _, loss_val = sess.run(\n\u001b[1;32m---> 48\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m                 {encoder_inputs: source_batch,\n\u001b[0;32m     50\u001b[0m                  \u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'updates' is not defined"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    lower_case_words = [word.lower() for word in sentence.split()]\n",
    "    \n",
    "    word_id = [vocab_to_int.get(word, vocab_to_int['<unk>']) for word in lower_case_words]\n",
    "    \n",
    "    return word_id\n",
    "\n",
    "import time\n",
    "\n",
    "### TRAINING ###\n",
    "save_path = 'checkpoints/dev'\n",
    "\n",
    "train_source = source_int_text[:]\n",
    "train_target = target_int_text[:]\n",
    "\n",
    "valid_source = source_int_text[:batch_size_train]\n",
    "valid_target = target_int_text[:batch_size_train]\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch) in enumerate(\n",
    "                batch_data(train_source, train_target, batch_size_train)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            source_batch_seq_lenght = []\n",
    "            for item in source_batch:\n",
    "                source_batch_seq_lenght.append(np.shape(item)[0])\n",
    "            \n",
    "            target_batch_seq_lenght = []\n",
    "            for item in target_batch:\n",
    "                target_batch_seq_lenght.append(np.shape(item)[0])\n",
    "                \n",
    "            print(source_batch_seq_lenght)\n",
    "            print(target_batch_seq_lenght)\n",
    "            \n",
    "            if (source_batch_seq_lenght[0] > 300):\n",
    "                continue\n",
    "                \n",
    "            _, loss_val = sess.run(\n",
    "                [updates, loss],\n",
    "                {encoder_inputs: source_batch,\n",
    "                 decoder_inputs: target_batch,\n",
    "                encoder_inputs_length: source_batch_seq_lenght,\n",
    "                decoder_inputs_length: target_batch_seq_lenght})\n",
    "\n",
    "            print('Epoch {:>3} Batch {:>4}/{}, Loss: {:>6.3f}'\n",
    "                  .format(epoch_i, batch_i, len(source_int_text) // batch_size_train, loss_val))\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "    print(\"Training time: \", end_time - start_time)\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "save_params(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "(16,)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "[[223 223 223 223 223 223 223 223 223 223]\n",
      " [146 146 146 146 146 146 146 146 146 146]\n",
      " [146 146 146 146 146 146 146 146 146 146]\n",
      " [181 181 181 181 181 181 181 181 181 181]\n",
      " [179 179 179 179 179 179 179 179 179 179]\n",
      " [ 76  76  76  76  76  76  76  76  76  76]\n",
      " [ 58  58  58  58  58  58  58  58  58  58]\n",
      " [ 30  30  30  30  30  30  30  30  30  30]\n",
      " [154 154 154 154 154 154 154 154 154 154]\n",
      " [ 88  88  88  88  88  88  88  88  88  88]\n",
      " [154 154 154 154 154 154 154 154 154 154]\n",
      " [ 88  88  88  88  88  88  88  88  88  88]\n",
      " [154 154 154 154 154 154 154 154 154 154]\n",
      " [ 70  70  70  70  70  70  70  70  70  70]\n",
      " [154 154 154 154 154 154 154 154 154 154]\n",
      " [ 88  88  88  88  88  88  88  88  88  88]\n",
      " [ 70  70  70  70  70  70  70  70  70  70]\n",
      " [ 88  88  88  88  88  88  88  88  88  88]\n",
      " [ 70  70  70  70  70  70  70  70  70  70]\n",
      " [ 70  70  70  70  70  70  70  70  70  70]\n",
      " [ 70  70  70  70  70  70  70  70  70  70]\n",
      " [ 70  70  88  70  88  70  70  88  70  88]\n",
      " [ 70  70  70  70  70  70  70  70  70  70]\n",
      " [ 88  88  70  88  70  88  88  70  88  70]\n",
      " [134 134  70 134  70 134 134  70  70  70]\n",
      " [ 88  88  70  88 154  88  88 154 134 154]\n",
      " [134 134  70 134 110 134 134 110  88 110]\n",
      " [134 134  70 134  70 134 134  70 134  70]\n",
      " [134  88 134  88 154  88  88 154 134 154]\n",
      " [ 88 134 134 134  88 134 134  88  88  88]\n",
      " [134 134  88  88 134  88 134 134 134 134]\n",
      " [ 88  88 134 134  88  88 134 123  88  70]]\n",
      "Input\n",
      "  Word Ids:      [49, 55, 306, 160, 168, 313, 61, 202, 281, 77, 307, 306, 209, 91, 346, 298]\n",
      "  Source Words: ['new', 'jersey', 'est', 'parfois', 'calme', 'pendant', \"l'\", 'automne', ',', 'et', 'il', 'est', 'neigeux', 'en', 'avril', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [223, 146, 146, 181, 179, 76, 58, 30, 154, 88, 154, 88, 154, 70, 154, 88, 70, 88, 70, 70, 70, 70, 70, 88, 134, 88, 134, 134, 134, 88, 134, 88]\n",
      "  Predicted Words: ['sharks', 'shark', 'shark', 'lime', 'snake', 'to', \"it's\", 'spanish', 'translate', 'chinese', 'translate', 'chinese', 'translate', 'easy', 'translate', 'chinese', 'easy', 'chinese', 'easy', 'easy', 'easy', 'easy', 'easy', 'chinese', 'lemons', 'chinese', 'lemons', 'lemons', 'lemons', 'chinese', 'lemons', 'chinese']\n",
      "\n",
      "Translation:\n",
      "\n",
      "apples rabbits rabbits rabbits lemon. lemon. paris it easy easy chinese easy easy easy easy easy easy easy easy easy easy easy chinese lemons easy chinese easy lemons chinese lemons lemons chinese \n"
     ]
    }
   ],
   "source": [
    "translate_sentence = \"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"\n",
    "#fr to en\n",
    "#input: \"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"\n",
    "#target:\"new jersey is sometimes quiet during autumn , and it is snowy in april .\"\n",
    "\n",
    "#en to vi\n",
    "#input:  \"Họ viết gần 1000 trang về chủ đề này .\"\n",
    "#target: \"They wrote almost a thousand pages on the topic .\"\n",
    "\n",
    "print(translate_sentence)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "print(np.shape(translate_sentence))\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    encoder_inputs = loaded_graph.get_tensor_by_name('encoder_inputs:0')\n",
    "    encoder_inputs_length = loaded_graph.get_tensor_by_name('encoder_inputs_length:0')\n",
    "    decoder_pred_decode = loaded_graph.get_tensor_by_name('decoder_pred_decode:0')\n",
    "    \n",
    "    predicted_ids = sess.run(decoder_pred_decode, {encoder_inputs: [translate_sentence],\n",
    "                                                       encoder_inputs_length: [np.shape(translate_sentence)[0]]})[0]\n",
    "    \n",
    "    print(predicted_ids)\n",
    "    \n",
    "\n",
    "# with tf.Session(graph=train_graph) as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     translate_logits = sess.run(decoder_pred_decode, {encoder_inputs: [translate_sentence],\n",
    "#                                                        encoder_inputs_length: [np.shape(translate_sentence)[0]]})[0]\n",
    "    \n",
    "#     print(translate_logits)\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  Source Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i[0] for i in predicted_ids]))\n",
    "print('  Predicted Words: {}'.format([target_int_to_vocab[i[0]] for i in predicted_ids]))\n",
    "\n",
    "print('\\nTranslation:\\n')\n",
    "translation = ''\n",
    "for word_i in translate_logits:\n",
    "    translation += target_int_to_vocab[word_i[0]] + ' '\n",
    "    \n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
