{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The helper file\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "CODES = {'<unk>': 0, '<s>': 1, '</s>': 2}\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_and_save_data(source_path, target_path):\n",
    "    \"\"\"\n",
    "    Preprocess Text Data.  Save to to file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess\n",
    "    source_text = load_data(source_path)\n",
    "    target_text = load_data(target_path)\n",
    "\n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "\n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "    \n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "    \n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "    # Save Data\n",
    "    pickle.dump((\n",
    "        (source_text, target_text),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))\n",
    "\n",
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    \"\"\"\n",
    "    vocab = set(text.split())\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "    \n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))\n",
    "\n",
    "def batch_data(source, target, batch_size):\n",
    "    \"\"\"\n",
    "    Batch source and target together\n",
    "    \"\"\"\n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = source[start_i:start_i + batch_size]\n",
    "        target_batch = target[start_i:start_i + batch_size]\n",
    "        yield np.array(pad_sentence_batch(source_batch)), np.array(pad_sentence_batch(target_batch))\n",
    "\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"\n",
    "    Pad sentence with </s> id\n",
    "    \"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [CODES['</s>']] * (max_sentence - len(sentence))\n",
    "            for sentence in sentence_batch]\n",
    "\n",
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    source_text_to_id = [[source_vocab_to_int[word] for word in line.split()] for line in source_text.split('\\n')]\n",
    "    target_text_to_id = [[target_vocab_to_int[word] for word in line.split()] for line in target_text.split('\\n')]\n",
    "    \n",
    "    return (source_text_to_id, target_text_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_path = 'data/small_vocab_fr'\n",
    "target_path = 'data/small_vocab_en'\n",
    "source_text = load_data(source_path)\n",
    "target_text = load_data(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocess_and_save_data(source_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denis\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "assert LooseVersion(tf.__version__) in [LooseVersion('1.4.0')], 'This project requires TensorFlow version 1.5  You are using {}'.format(tf.__version__)\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "# pad_sentence_batch(source_int_text)\n",
    "source_vocab = len(source_vocab_to_int)\n",
    "target_vocab = len(target_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seqHyperparams(object):\n",
    "    def __init__(self, hidden_units=256, n_layers_enconder=2,\n",
    "                 n_layers_decoder=2, num_encoder_symbols=source_vocab, \n",
    "                 num_decoder_symbols=target_vocab, learning_rate=0.01,\n",
    "                 embedding_size=15, max_gradient_norm=5.0, dtype=tf.float32,\n",
    "                 epochs=1, dropout=0.2, forget_bias=1.0,\n",
    "                 use_beam_search=True, beam_width=10, length_penalty_weight=0.0,\n",
    "                 use_attention=True, learning_rate_decay=False, \n",
    "                 use_bidirectional_enconder=False):\n",
    "    \n",
    "        self.hidden_units = hidden_units\n",
    "        self.n_layers_enconder = n_layers_enconder\n",
    "        self.n_layers_decoder = n_layers_decoder\n",
    "        self.num_encoder_symbols = num_encoder_symbols\n",
    "        self.num_decoder_symbols = num_decoder_symbols\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.dtype = dtype\n",
    "        self.dropout = dropout\n",
    "        self.forget_bias = forget_bias\n",
    "        self.use_beam_search = use_beam_search\n",
    "        self.beam_width = beam_width\n",
    "        self.length_penalty_weight = length_penalty_weight\n",
    "        self.use_attention = use_attention\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.use_bidirectional_enconder = use_bidirectional_enconder\n",
    "\n",
    "\n",
    "        # Extra vocabulary symbols\n",
    "        unk = '<unk>'\n",
    "        sos = '<s>'\n",
    "        eos = '</s>' # also function as PAD\n",
    "        self.extra_tokens = [unk, sos, eos]\n",
    "        self.unk_token = self.extra_tokens.index(unk) #unk_token = 0\n",
    "        self.start_token = self.extra_tokens.index(sos) # start_token = 1\n",
    "        self.end_token = self.extra_tokens.index(eos)   # end_token = 2\n",
    "\n",
    "hparams = Seq2seqHyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.contrib.rnn import MultiRNNCell\n",
    "from tensorflow import layers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    ### DEFINING PLACEHOLDERS ###\n",
    "\n",
    "    # encoder_inputs: [batch_size, max_time_steps]\n",
    "    encoder_inputs = tf.placeholder(dtype=tf.int32,\n",
    "                shape=(None, None), name='encoder_inputs')\n",
    "\n",
    "    # encoder_inputs_length: [batch_size]\n",
    "    encoder_inputs_length = tf.placeholder(\n",
    "                dtype=tf.int32, shape=(None,), name='encoder_inputs_length')\n",
    "\n",
    "    # get dynamic batch_size\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "\n",
    "    ### TRAIN MODE PLACEHOLDERS ###\n",
    "\n",
    "    # decoder_inputs: [batch_size, max_time_steps]\n",
    "    decoder_inputs = tf.placeholder(\n",
    "                    dtype=tf.int32, shape=(None, None), name='decoder_inputs')\n",
    "\n",
    "    # decoder_inputs_length: [batch_size]\n",
    "    decoder_inputs_length = tf.placeholder(\n",
    "                    dtype=tf.int32, shape=(None,), name='decoder_inputs_length')\n",
    "\n",
    "    decoder_start_token = tf.ones(\n",
    "                    shape=[batch_size, 1], dtype=tf.int32) * hparams.start_token\n",
    "    decoder_end_token = tf.ones(\n",
    "                    shape=[batch_size, 1], dtype=tf.int32) * hparams.end_token  \n",
    "\n",
    "\n",
    "    # decoder_inputs_train: [batch_size , max_time_steps + 1]\n",
    "    # insert sos symbol in front of each decoder input\n",
    "    decoder_inputs_train = tf.concat([decoder_start_token,\n",
    "                                          decoder_inputs], axis=1)\n",
    "\n",
    "    # decoder_inputs_length_train: [batch_size]\n",
    "    decoder_inputs_length_train = decoder_inputs_length + 1\n",
    "\n",
    "    # decoder_targets_train: [batch_size, max_time_steps + 1]\n",
    "    # insert eos symbol at the end of each decoder input\n",
    "    decoder_targets_train = tf.concat([decoder_inputs,\n",
    "                                           decoder_end_token], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## DEFINING ENCODER ##\n",
    "\n",
    "    encoder_embeddings = tf.Variable(tf.random_uniform([hparams.num_encoder_symbols, hparams.embedding_size], -1.0, 1.0),\n",
    "                                     dtype=hparams.dtype)\n",
    "\n",
    "    # Embedded_inputs: [batch_size, time_step, embedding_size]\n",
    "    encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "        params=encoder_embeddings, ids=encoder_inputs)\n",
    "\n",
    "    if hparams.use_bidirectional_enconder:  #bidirectional encoder is not working!\n",
    "        \n",
    "        num_bi_layers = int(hparams.n_layers_enconder / 2)\n",
    "        num_residual_layers = hparams.n_layers_enconder - 1\n",
    "        num_bi_residual_layers = int(num_residual_layers / 2)\n",
    "        \n",
    "        print(num_bi_layers, num_residual_layers, num_bi_residual_layers)\n",
    "        \n",
    "        cell_list = []\n",
    "        for i in range(hparams.n_layers_enconder):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(hparams.hidden_units, forget_bias=hparams.forget_bias)\n",
    "\n",
    "            if (i >= hparams.n_layers_enconder - num_residual_layers):\n",
    "                cell = tf.contrib.rnn.ResidualWrapper(cell, residual_fn=None)\n",
    "                if hparams.dropout > 0.0:\n",
    "                    cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                        cell=cell, input_keep_prob=(1.0 - hparams.dropout))\n",
    "            \n",
    "            cell_list.append(cell)\n",
    "            \n",
    "        if len(cell_list) == 1:  # Single layer.\n",
    "            fw_cell = cell_list[0]\n",
    "            bw_cell = cell_list[0]\n",
    "        else:  # Multi layers\n",
    "            fw_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "            bw_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "\n",
    "        fw_cell = tf.contrib.rnn.BasicLSTMCell(hparams.n_layers_enconder)\n",
    "        bw_cell = tf.contrib.rnn.BasicLSTMCell(hparams.n_layers_enconder)\n",
    "\n",
    "        bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                                        fw_cell,\n",
    "                                                        bw_cell,\n",
    "                                                        encoder_inputs_embedded,\n",
    "                                                        dtype=dtype,\n",
    "                                                        sequence_length=encoder_inputs_length,\n",
    "                                                        time_major=False,\n",
    "                                                        swap_memory=True)\n",
    "        print(bi_outputs, \"\\n\\n\", bi_state)\n",
    "\n",
    "        encoder_outputs, bi_encoder_state = tf.concat(bi_outputs, -1), bi_state\n",
    "        \n",
    "        if num_bi_layers == 1:\n",
    "            encoder_last_state = bi_encoder_state\n",
    "        else:\n",
    "            # alternatively concat forward and backward states\n",
    "            encoder_state = []\n",
    "            for layer_id in range(num_bi_layers):\n",
    "                encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "                encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "            encoder_last_state = tuple(encoder_state)\n",
    "\n",
    "        encoder_state = bi_encoder_state\n",
    "        \n",
    "    else:\n",
    "        # Build RNN cell\n",
    "        cells = []\n",
    "        for _ in range(hparams.n_layers_enconder):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(hparams.hidden_units, forget_bias=hparams.forget_bias)\n",
    "            if hparams.dropout > 0.0:\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                    cell=cell, input_keep_prob=(1.0 - hparams.dropout))\n",
    "            cells.append(cell)\n",
    "        if hparams.n_layers_enconder == 1:\n",
    "            encoder_cells = cells[0]\n",
    "        else:\n",
    "            encoder_cells = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "        encoder_outputs, encoder_last_state = tf.nn.dynamic_rnn(\n",
    "            cell=encoder_cells, inputs=encoder_inputs_embedded,\n",
    "            sequence_length=encoder_inputs_length, dtype=hparams.dtype,\n",
    "            time_major=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ### DEFINING DECODER ###\n",
    "\n",
    "    # Building decoder_cell\n",
    "    cells = []\n",
    "    # Build RNN cell\n",
    "    for _ in range(hparams.n_layers_decoder):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(hparams.hidden_units, forget_bias=hparams.forget_bias)\n",
    "        if hparams.dropout > 0.0:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell=cell, input_keep_prob=(1.0 - hparams.dropout))\n",
    "        cells.append(cell)\n",
    "    if hparams.n_layers_decoder == 1:\n",
    "        decoder_cells = cells[0]\n",
    "    else:\n",
    "        decoder_cells = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "    if hparams.use_attention:\n",
    "        memory = encoder_outputs\n",
    "        \n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "            hparams.hidden_units,\n",
    "            memory,\n",
    "            memory_sequence_length=encoder_inputs_length,\n",
    "            normalize=True)\n",
    "        \n",
    "        decoder_cells_train = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            decoder_cells,\n",
    "            attention_mechanism,\n",
    "            attention_layer_size=hparams.hidden_units,\n",
    "            alignment_history=False,\n",
    "            output_attention=True,\n",
    "            name=\"attention\")\n",
    "        \n",
    "        decoder_initial_state = decoder_cells_train.zero_state(batch_size, hparams.dtype).clone(\n",
    "          cell_state=encoder_last_state)\n",
    "        \n",
    "    else:\n",
    "        decoder_cells_train = decoder_cells\n",
    "        decoder_initial_state = encoder_last_state\n",
    "\n",
    "    decoder_embeddings = tf.Variable(tf.random_uniform([hparams.num_decoder_symbols, hparams.embedding_size], -1.0, 1.0), dtype=hparams.dtype)\n",
    "    \n",
    "    # decoder_inputs_embedded: [batch_size, max_time_step + 1, embedding_size]\n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "        params=decoder_embeddings, ids=decoder_inputs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ### TRAIN MODE ###\n",
    "    \n",
    "    # Helper to feed inputs for training: read inputs from dense ground truth vectors\n",
    "    training_helper = seq2seq.TrainingHelper(inputs=decoder_inputs_embedded,\n",
    "                                       sequence_length=decoder_inputs_length_train,\n",
    "                                       time_major=False,\n",
    "                                        name='training_helper')\n",
    "\n",
    "    training_decoder = seq2seq.BasicDecoder(cell=decoder_cells_train,\n",
    "                                       helper=training_helper,\n",
    "                                       initial_state=decoder_initial_state)\n",
    "\n",
    "    # decoder_outputs_train: BasicDecoderOutput\n",
    "    #                        namedtuple(rnn_outputs, sample_id)\n",
    "    # decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, num_decoder_symbols] if output_time_major=False\n",
    "    #                                   [max_time_step + 1, batch_size, num_decoder_symbols] if output_time_major=True\n",
    "    # decoder_outputs_train.sample_id: [batch_size], tf.int32\n",
    "    (decoder_outputs_train, decoder_last_state_train, \n",
    "         decoder_outputs_length_decode)  = seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                        output_time_major=False,\n",
    "                                                        swap_memory=True,\n",
    "                                                        impute_finished=True)\n",
    "\n",
    "    # More efficient to do the projection on the batch-time-concatenated tensor\n",
    "    # logits_train: [batch_size, max_time_step + 1, num_decoder_symbols]\n",
    "    \n",
    "    sample_id = decoder_outputs_train.sample_id\n",
    "    \n",
    "    output_layer = layers.Dense(hparams.num_decoder_symbols, name='output_projection')\n",
    "    logits_train = output_layer(decoder_outputs_train.rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    \n",
    "    ### LOSS, GRADIEND AND OPTIMIZATION ###\n",
    "    \n",
    "    if hparams.learning_rate_decay:\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        learning_rate = tf.constant(hparams.learning_rate)\n",
    "\n",
    "        #using luong10 decay scheme\n",
    "        decay_factor = 0.5\n",
    "        start_decay_step = int(hparams.epochs / 2)\n",
    "        decay_times = 10\n",
    "\n",
    "        remain_steps = hparams.epochs - start_decay_step\n",
    "        decay_steps = int(remain_steps / decay_times)\n",
    "\n",
    "        learning_rate = tf.cond(global_step < start_decay_step,\n",
    "                                lambda: hparams.learning_rate,\n",
    "                                lambda: tf.train.exponential_decay(\n",
    "                                    hparams.learning_rate,\n",
    "                                    (global_step - start_decay_step),\n",
    "                                    decay_steps, decay_factor, staircase=True),\n",
    "                                name=\"learning_rate_decay_cond\")\n",
    "    \n",
    "    # Maximum decoder time_steps in current batch\n",
    "    max_decoder_length = tf.reduce_max(decoder_inputs_length_train)\n",
    "    \n",
    "    # masks: masking for valid and padded time steps, [batch_size, max_time_step + 1]\n",
    "    target_weights = tf.sequence_mask(lengths=decoder_inputs_length_train, \n",
    "                             maxlen=max_decoder_length, dtype=hparams.dtype, name='masks')\n",
    "    \n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=decoder_targets_train, logits=logits_train)\n",
    "    \n",
    "    loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "        tf.cast(batch_size, dtype=hparams.dtype))\n",
    "\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
    "    \n",
    "    gradients = tf.gradients(loss, \n",
    "                             trainable_params)\n",
    "    \n",
    "    clip_gradients, gradient_norm = tf.clip_by_global_norm(gradients, hparams.max_gradient_norm)\n",
    "    \n",
    "    updates = opt.apply_gradients(\n",
    "            zip(clip_gradients, trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "\n",
    "    ### INFERENCE MODE ###\n",
    "    start_tokens = tf.fill([batch_size], hparams.start_token)\n",
    "    \n",
    "    decoder_initial_state_infer = tf.contrib.seq2seq.tile_batch(\n",
    "                  encoder_last_state, multiplier=hparams.beam_width)\n",
    "    \n",
    "    if hparams.use_attention:\n",
    "        memory = tf.contrib.seq2seq.tile_batch(\n",
    "          memory, multiplier=hparams.beam_width)\n",
    "        \n",
    "        source_sequence_length = tf.contrib.seq2seq.tile_batch(\n",
    "          encoder_inputs_length, multiplier=hparams.beam_width)\n",
    "        \n",
    "        encoder_last_state = tf.contrib.seq2seq.tile_batch(\n",
    "          encoder_last_state, multiplier=hparams.beam_width)\n",
    "        \n",
    "        batch_size = batch_size * hparams.beam_width\n",
    "        \n",
    "        attention_mechanism_infer = tf.contrib.seq2seq.BahdanauAttention(\n",
    "            hparams.hidden_units,\n",
    "            memory,\n",
    "            memory_sequence_length=source_sequence_length,\n",
    "            normalize=True)\n",
    "        \n",
    "        decoder_cells_infer = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            decoder_cells,\n",
    "            attention_mechanism_infer,\n",
    "            attention_layer_size=hparams.hidden_units,\n",
    "            alignment_history=False,\n",
    "            output_attention=True,\n",
    "            name=\"attention_infer\")\n",
    "        \n",
    "        decoder_initial_state_infer = decoder_cells_infer.zero_state(batch_size, hparams.dtype).clone(\n",
    "          cell_state=encoder_last_state)\n",
    "    \n",
    "    if hparams.use_beam_search:\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "              cell=decoder_cells_infer,\n",
    "              embedding=decoder_embeddings,\n",
    "              start_tokens=start_tokens,\n",
    "              end_token=hparams.end_token,\n",
    "              initial_state=decoder_initial_state_infer,\n",
    "              beam_width=hparams.beam_width,\n",
    "              output_layer=output_layer,\n",
    "              length_penalty_weight=hparams.length_penalty_weight)\n",
    "        \n",
    "    else:\n",
    "        inference_helper = seq2seq.GreedyEmbeddingHelper(decoder_embeddings,\n",
    "                                                        start_tokens=start_tokens,\n",
    "                                                        end_token=hparams.end_token)\n",
    "\n",
    "        inference_decoder = seq2seq.BasicDecoder(cell=decoder_cells_infer,\n",
    "                                                 helper=inference_helper,\n",
    "                                                 initial_state=decoder_initial_state,\n",
    "                                                 output_layer=output_layer)\n",
    "    \n",
    "    maximum_iterations = tf.round(tf.reduce_max(encoder_inputs_length) * 2)\n",
    "    \n",
    "    (decoder_infer_outputs, decoder_infer_last_state,\n",
    "                 decoder_infer_outputs_length) = (seq2seq.dynamic_decode(\n",
    "                    decoder=inference_decoder,\n",
    "                    output_time_major=False,\n",
    "                    maximum_iterations=maximum_iterations))\n",
    "    \n",
    "    if hparams.use_beam_search:\n",
    "        decoder_pred_decode = decoder_infer_outputs.predicted_ids\n",
    "        tf.identity(decoder_pred_decode, 'decoder_pred_decode')\n",
    "    \n",
    "    else:\n",
    "        logits_infer = decoder_infer_outputs.rnn_output\n",
    "        sample_id_infer = decoder_infer_outputs.sample_id                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training parameters\n",
    "class TrainingHyperparams(object):\n",
    "    def __init__(self, epochs=3, batch_size=512):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "train_hparams = TrainingHyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/269, Loss: 97.822\n",
      "Epoch   0 Batch    1/269, Loss: 77.312\n",
      "Epoch   0 Batch    2/269, Loss: 435.758\n",
      "Epoch   0 Batch    3/269, Loss: 95.297\n",
      "Epoch   0 Batch    4/269, Loss: 143.852\n",
      "Epoch   0 Batch    5/269, Loss: 92.283\n",
      "Epoch   0 Batch    6/269, Loss: 78.013\n",
      "Epoch   0 Batch    7/269, Loss: 79.135\n",
      "Epoch   0 Batch    8/269, Loss: 67.675\n",
      "Epoch   0 Batch    9/269, Loss: 67.247\n",
      "Epoch   0 Batch   10/269, Loss: 61.134\n",
      "Epoch   0 Batch   11/269, Loss: 59.242\n",
      "Epoch   0 Batch   12/269, Loss: 56.646\n",
      "Epoch   0 Batch   13/269, Loss: 54.664\n",
      "Epoch   0 Batch   14/269, Loss: 55.011\n",
      "Epoch   0 Batch   15/269, Loss: 54.845\n",
      "Epoch   0 Batch   16/269, Loss: 51.762\n",
      "Epoch   0 Batch   17/269, Loss: 51.130\n",
      "Epoch   0 Batch   18/269, Loss: 49.760\n",
      "Epoch   0 Batch   19/269, Loss: 53.007\n",
      "Epoch   0 Batch   20/269, Loss: 48.209\n",
      "Epoch   0 Batch   21/269, Loss: 50.687\n",
      "Epoch   0 Batch   22/269, Loss: 48.596\n",
      "Epoch   0 Batch   23/269, Loss: 46.859\n",
      "Epoch   0 Batch   24/269, Loss: 50.131\n",
      "Epoch   0 Batch   25/269, Loss: 47.803\n",
      "Epoch   0 Batch   26/269, Loss: 47.753\n",
      "Epoch   0 Batch   27/269, Loss: 47.207\n",
      "Epoch   0 Batch   28/269, Loss: 47.277\n",
      "Epoch   0 Batch   29/269, Loss: 45.934\n",
      "Epoch   0 Batch   30/269, Loss: 44.660\n",
      "Epoch   0 Batch   31/269, Loss: 43.231\n",
      "Epoch   0 Batch   32/269, Loss: 42.489\n",
      "Epoch   0 Batch   33/269, Loss: 41.405\n",
      "Epoch   0 Batch   34/269, Loss: 41.906\n",
      "Epoch   0 Batch   35/269, Loss: 41.060\n",
      "Epoch   0 Batch   36/269, Loss: 40.313\n",
      "Epoch   0 Batch   37/269, Loss: 39.748\n",
      "Epoch   0 Batch   38/269, Loss: 38.694\n",
      "Epoch   0 Batch   39/269, Loss: 38.379\n",
      "Epoch   0 Batch   40/269, Loss: 36.750\n",
      "Epoch   0 Batch   41/269, Loss: 35.834\n",
      "Epoch   0 Batch   42/269, Loss: 35.695\n",
      "Epoch   0 Batch   43/269, Loss: 35.381\n",
      "Epoch   0 Batch   44/269, Loss: 34.580\n",
      "Epoch   0 Batch   45/269, Loss: 34.325\n",
      "Epoch   0 Batch   46/269, Loss: 33.447\n",
      "Epoch   0 Batch   47/269, Loss: 34.442\n",
      "Epoch   0 Batch   48/269, Loss: 31.165\n",
      "Epoch   0 Batch   49/269, Loss: 32.265\n",
      "Epoch   0 Batch   50/269, Loss: 30.872\n",
      "Epoch   0 Batch   51/269, Loss: 33.246\n",
      "Epoch   0 Batch   52/269, Loss: 30.601\n",
      "Epoch   0 Batch   53/269, Loss: 28.873\n",
      "Epoch   0 Batch   54/269, Loss: 28.182\n",
      "Epoch   0 Batch   55/269, Loss: 28.535\n",
      "Epoch   0 Batch   56/269, Loss: 26.316\n",
      "Epoch   0 Batch   57/269, Loss: 27.511\n",
      "Epoch   0 Batch   58/269, Loss: 27.733\n",
      "Epoch   0 Batch   59/269, Loss: 25.795\n",
      "Epoch   0 Batch   60/269, Loss: 27.224\n",
      "Epoch   0 Batch   61/269, Loss: 24.874\n",
      "Epoch   0 Batch   62/269, Loss: 25.556\n",
      "Epoch   0 Batch   63/269, Loss: 26.535\n",
      "Epoch   0 Batch   64/269, Loss: 24.509\n",
      "Epoch   0 Batch   65/269, Loss: 24.133\n",
      "Epoch   0 Batch   66/269, Loss: 25.706\n",
      "Epoch   0 Batch   67/269, Loss: 24.183\n",
      "Epoch   0 Batch   68/269, Loss: 23.189\n",
      "Epoch   0 Batch   69/269, Loss: 22.817\n",
      "Epoch   0 Batch   70/269, Loss: 22.789\n",
      "Epoch   0 Batch   71/269, Loss: 21.604\n",
      "Epoch   0 Batch   72/269, Loss: 21.036\n",
      "Epoch   0 Batch   73/269, Loss: 20.534\n",
      "Epoch   0 Batch   74/269, Loss: 19.484\n",
      "Epoch   0 Batch   75/269, Loss: 19.156\n",
      "Epoch   0 Batch   76/269, Loss: 18.976\n",
      "Epoch   0 Batch   77/269, Loss: 18.398\n",
      "Epoch   0 Batch   78/269, Loss: 18.085\n",
      "Epoch   0 Batch   79/269, Loss: 17.261\n",
      "Epoch   0 Batch   80/269, Loss: 17.157\n",
      "Epoch   0 Batch   81/269, Loss: 17.539\n",
      "Epoch   0 Batch   82/269, Loss: 16.017\n",
      "Epoch   0 Batch   83/269, Loss: 16.247\n",
      "Epoch   0 Batch   84/269, Loss: 15.974\n",
      "Epoch   0 Batch   85/269, Loss: 15.728\n",
      "Epoch   0 Batch   86/269, Loss: 15.579\n",
      "Epoch   0 Batch   87/269, Loss: 15.431\n",
      "Epoch   0 Batch   88/269, Loss: 15.232\n",
      "Epoch   0 Batch   89/269, Loss: 15.198\n",
      "Epoch   0 Batch   90/269, Loss: 15.216\n",
      "Epoch   0 Batch   91/269, Loss: 14.519\n",
      "Epoch   0 Batch   92/269, Loss: 14.755\n",
      "Epoch   0 Batch   93/269, Loss: 14.573\n",
      "Epoch   0 Batch   94/269, Loss: 14.686\n",
      "Epoch   0 Batch   95/269, Loss: 14.114\n",
      "Epoch   0 Batch   96/269, Loss: 15.109\n",
      "Epoch   0 Batch   97/269, Loss: 24.340\n",
      "Epoch   0 Batch   98/269, Loss: 17.931\n",
      "Epoch   0 Batch   99/269, Loss: 19.920\n",
      "Epoch   0 Batch  100/269, Loss: 17.745\n",
      "Epoch   0 Batch  101/269, Loss: 15.489\n",
      "Epoch   0 Batch  102/269, Loss: 15.487\n",
      "Epoch   0 Batch  103/269, Loss: 14.850\n",
      "Epoch   0 Batch  104/269, Loss: 14.659\n",
      "Epoch   0 Batch  105/269, Loss: 14.803\n",
      "Epoch   0 Batch  106/269, Loss: 14.032\n",
      "Epoch   0 Batch  107/269, Loss: 13.832\n",
      "Epoch   0 Batch  108/269, Loss: 13.925\n",
      "Epoch   0 Batch  109/269, Loss: 13.968\n",
      "Epoch   0 Batch  110/269, Loss: 13.548\n",
      "Epoch   0 Batch  111/269, Loss: 13.670\n",
      "Epoch   0 Batch  112/269, Loss: 13.907\n",
      "Epoch   0 Batch  113/269, Loss: 13.637\n",
      "Epoch   0 Batch  114/269, Loss: 13.553\n",
      "Epoch   0 Batch  115/269, Loss: 13.465\n",
      "Epoch   0 Batch  116/269, Loss: 13.313\n",
      "Epoch   0 Batch  117/269, Loss: 13.312\n",
      "Epoch   0 Batch  118/269, Loss: 13.042\n",
      "Epoch   0 Batch  119/269, Loss: 12.867\n",
      "Epoch   0 Batch  120/269, Loss: 13.147\n",
      "Epoch   0 Batch  121/269, Loss: 13.151\n",
      "Epoch   0 Batch  122/269, Loss: 12.754\n",
      "Epoch   0 Batch  123/269, Loss: 12.959\n",
      "Epoch   0 Batch  124/269, Loss: 12.778\n",
      "Epoch   0 Batch  125/269, Loss: 13.094\n",
      "Epoch   0 Batch  126/269, Loss: 12.807\n",
      "Epoch   0 Batch  127/269, Loss: 12.819\n",
      "Epoch   0 Batch  128/269, Loss: 12.795\n",
      "Epoch   0 Batch  129/269, Loss: 12.694\n",
      "Epoch   0 Batch  130/269, Loss: 12.438\n",
      "Epoch   0 Batch  131/269, Loss: 12.631\n",
      "Epoch   0 Batch  132/269, Loss: 12.857\n",
      "Epoch   0 Batch  133/269, Loss: 12.346\n",
      "Epoch   0 Batch  134/269, Loss: 12.174\n",
      "Epoch   0 Batch  135/269, Loss: 12.765\n",
      "Epoch   0 Batch  136/269, Loss: 12.649\n",
      "Epoch   0 Batch  137/269, Loss: 12.456\n",
      "Epoch   0 Batch  138/269, Loss: 12.351\n",
      "Epoch   0 Batch  139/269, Loss: 12.018\n",
      "Epoch   0 Batch  140/269, Loss: 12.473\n",
      "Epoch   0 Batch  141/269, Loss: 12.039\n",
      "Epoch   0 Batch  142/269, Loss: 12.520\n",
      "Epoch   0 Batch  143/269, Loss: 12.281\n",
      "Epoch   0 Batch  144/269, Loss: 12.128\n",
      "Epoch   0 Batch  145/269, Loss: 12.087\n",
      "Epoch   0 Batch  146/269, Loss: 12.178\n",
      "Epoch   0 Batch  147/269, Loss: 12.465\n",
      "Epoch   0 Batch  148/269, Loss: 12.054\n",
      "Epoch   0 Batch  149/269, Loss: 11.885\n",
      "Epoch   0 Batch  150/269, Loss: 11.857\n",
      "Epoch   0 Batch  151/269, Loss: 11.900\n",
      "Epoch   0 Batch  152/269, Loss: 11.854\n",
      "Epoch   0 Batch  153/269, Loss: 11.646\n",
      "Epoch   0 Batch  154/269, Loss: 11.554\n",
      "Epoch   0 Batch  155/269, Loss: 11.874\n",
      "Epoch   0 Batch  156/269, Loss: 11.611\n",
      "Epoch   0 Batch  157/269, Loss: 11.724\n",
      "Epoch   0 Batch  158/269, Loss: 11.615\n",
      "Epoch   0 Batch  159/269, Loss: 11.915\n",
      "Epoch   0 Batch  160/269, Loss: 11.150\n",
      "Epoch   0 Batch  161/269, Loss: 11.378\n",
      "Epoch   0 Batch  162/269, Loss: 11.550\n",
      "Epoch   0 Batch  163/269, Loss: 11.434\n",
      "Epoch   0 Batch  164/269, Loss: 11.058\n",
      "Epoch   0 Batch  165/269, Loss: 11.328\n",
      "Epoch   0 Batch  166/269, Loss: 11.580\n",
      "Epoch   0 Batch  167/269, Loss: 11.496\n",
      "Epoch   0 Batch  168/269, Loss: 11.132\n",
      "Epoch   0 Batch  169/269, Loss: 11.358\n",
      "Epoch   0 Batch  170/269, Loss: 11.054\n",
      "Epoch   0 Batch  171/269, Loss: 11.470\n",
      "Epoch   0 Batch  172/269, Loss: 11.436\n",
      "Epoch   0 Batch  173/269, Loss: 10.948\n",
      "Epoch   0 Batch  174/269, Loss: 10.868\n",
      "Epoch   0 Batch  175/269, Loss: 11.160\n",
      "Epoch   0 Batch  176/269, Loss: 10.989\n",
      "Epoch   0 Batch  177/269, Loss: 10.929\n",
      "Epoch   0 Batch  178/269, Loss: 10.721\n",
      "Epoch   0 Batch  179/269, Loss: 10.945\n",
      "Epoch   0 Batch  180/269, Loss: 10.732\n",
      "Epoch   0 Batch  181/269, Loss: 10.645\n",
      "Epoch   0 Batch  182/269, Loss: 10.666\n",
      "Epoch   0 Batch  183/269, Loss: 11.429\n",
      "Epoch   0 Batch  184/269, Loss: 11.439\n",
      "Epoch   0 Batch  185/269, Loss: 10.729\n",
      "Epoch   0 Batch  186/269, Loss: 10.877\n",
      "Epoch   0 Batch  187/269, Loss: 10.743\n",
      "Epoch   0 Batch  188/269, Loss: 10.609\n",
      "Epoch   0 Batch  189/269, Loss: 10.566\n",
      "Epoch   0 Batch  190/269, Loss: 10.625\n",
      "Epoch   0 Batch  191/269, Loss: 10.504\n",
      "Epoch   0 Batch  192/269, Loss: 10.136\n",
      "Epoch   0 Batch  193/269, Loss: 10.236\n",
      "Epoch   0 Batch  194/269, Loss: 10.534\n",
      "Epoch   0 Batch  195/269, Loss: 10.317\n",
      "Epoch   0 Batch  196/269, Loss: 10.182\n",
      "Epoch   0 Batch  197/269, Loss: 10.231\n",
      "Epoch   0 Batch  198/269, Loss:  9.915\n",
      "Epoch   0 Batch  199/269, Loss:  9.919\n",
      "Epoch   0 Batch  200/269, Loss:  9.909\n",
      "Epoch   0 Batch  201/269, Loss:  9.670\n",
      "Epoch   0 Batch  202/269, Loss:  9.842\n",
      "Epoch   0 Batch  203/269, Loss:  9.539\n",
      "Epoch   0 Batch  204/269, Loss:  9.670\n",
      "Epoch   0 Batch  205/269, Loss:  9.609\n",
      "Epoch   0 Batch  206/269, Loss:  9.313\n",
      "Epoch   0 Batch  207/269, Loss:  9.637\n",
      "Epoch   0 Batch  208/269, Loss:  9.156\n",
      "Epoch   0 Batch  209/269, Loss:  8.880\n",
      "Epoch   0 Batch  210/269, Loss:  9.082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  211/269, Loss:  8.951\n",
      "Epoch   0 Batch  212/269, Loss:  8.942\n",
      "Epoch   0 Batch  213/269, Loss:  8.937\n",
      "Epoch   0 Batch  214/269, Loss:  8.878\n",
      "Epoch   0 Batch  215/269, Loss:  8.445\n",
      "Epoch   0 Batch  216/269, Loss:  8.572\n",
      "Epoch   0 Batch  217/269, Loss:  8.461\n",
      "Epoch   0 Batch  218/269, Loss:  8.319\n",
      "Epoch   0 Batch  219/269, Loss:  8.438\n",
      "Epoch   0 Batch  220/269, Loss:  8.688\n",
      "Epoch   0 Batch  221/269, Loss:  7.845\n",
      "Epoch   0 Batch  222/269, Loss:  7.825\n",
      "Epoch   0 Batch  223/269, Loss:  7.873\n",
      "Epoch   0 Batch  224/269, Loss:  7.695\n",
      "Epoch   0 Batch  225/269, Loss:  7.322\n",
      "Epoch   0 Batch  226/269, Loss:  7.108\n",
      "Epoch   0 Batch  227/269, Loss:  7.495\n",
      "Epoch   0 Batch  228/269, Loss:  6.896\n",
      "Epoch   0 Batch  229/269, Loss:  6.858\n",
      "Epoch   0 Batch  230/269, Loss:  6.703\n",
      "Epoch   0 Batch  231/269, Loss:  6.551\n",
      "Epoch   0 Batch  232/269, Loss:  5.928\n",
      "Epoch   0 Batch  233/269, Loss:  5.809\n",
      "Epoch   0 Batch  234/269, Loss:  5.749\n",
      "Epoch   0 Batch  235/269, Loss:  5.785\n",
      "Epoch   0 Batch  236/269, Loss:  5.136\n",
      "Epoch   0 Batch  237/269, Loss:  4.915\n",
      "Epoch   0 Batch  238/269, Loss:  4.466\n",
      "Epoch   0 Batch  239/269, Loss:  4.313\n",
      "Epoch   0 Batch  240/269, Loss:  4.167\n",
      "Epoch   0 Batch  241/269, Loss:  4.174\n",
      "Epoch   0 Batch  242/269, Loss:  3.465\n",
      "Epoch   0 Batch  243/269, Loss:  3.237\n",
      "Epoch   0 Batch  244/269, Loss:  3.195\n",
      "Epoch   0 Batch  245/269, Loss:  3.166\n",
      "Epoch   0 Batch  246/269, Loss:  2.908\n",
      "Epoch   0 Batch  247/269, Loss:  2.735\n",
      "Epoch   0 Batch  248/269, Loss:  2.583\n",
      "Epoch   0 Batch  249/269, Loss:  2.467\n",
      "Epoch   0 Batch  250/269, Loss:  2.338\n",
      "Epoch   0 Batch  251/269, Loss:  2.218\n",
      "Epoch   0 Batch  252/269, Loss:  2.087\n",
      "Epoch   0 Batch  253/269, Loss:  2.194\n",
      "Epoch   0 Batch  254/269, Loss:  2.001\n",
      "Epoch   0 Batch  255/269, Loss:  2.216\n",
      "Epoch   0 Batch  256/269, Loss:  1.945\n",
      "Epoch   0 Batch  257/269, Loss:  2.003\n",
      "Epoch   0 Batch  258/269, Loss:  1.875\n",
      "Epoch   0 Batch  259/269, Loss:  1.909\n",
      "Epoch   0 Batch  260/269, Loss:  1.881\n",
      "Epoch   0 Batch  261/269, Loss:  1.680\n",
      "Epoch   0 Batch  262/269, Loss:  1.845\n",
      "Epoch   0 Batch  263/269, Loss:  1.899\n",
      "Epoch   0 Batch  264/269, Loss:  1.739\n",
      "Epoch   0 Batch  265/269, Loss:  1.432\n",
      "Epoch   0 Batch  266/269, Loss:  1.570\n",
      "Epoch   0 Batch  267/269, Loss:  1.647\n",
      "Epoch   1 Batch    0/269, Loss:  1.556\n",
      "Epoch   1 Batch    1/269, Loss:  1.432\n",
      "Epoch   1 Batch    2/269, Loss:  1.605\n",
      "Epoch   1 Batch    3/269, Loss:  1.576\n",
      "Epoch   1 Batch    4/269, Loss:  1.372\n",
      "Epoch   1 Batch    5/269, Loss:  1.428\n",
      "Epoch   1 Batch    6/269, Loss:  1.445\n",
      "Epoch   1 Batch    7/269, Loss:  1.416\n",
      "Epoch   1 Batch    8/269, Loss:  1.415\n",
      "Epoch   1 Batch    9/269, Loss:  1.428\n",
      "Epoch   1 Batch   10/269, Loss:  1.237\n",
      "Epoch   1 Batch   11/269, Loss:  1.421\n",
      "Epoch   1 Batch   12/269, Loss:  1.365\n",
      "Epoch   1 Batch   13/269, Loss:  1.229\n",
      "Epoch   1 Batch   14/269, Loss:  1.462\n",
      "Epoch   1 Batch   15/269, Loss:  1.272\n",
      "Epoch   1 Batch   16/269, Loss:  1.277\n",
      "Epoch   1 Batch   17/269, Loss:  1.256\n",
      "Epoch   1 Batch   18/269, Loss:  1.211\n",
      "Epoch   1 Batch   19/269, Loss:  1.409\n",
      "Epoch   1 Batch   20/269, Loss:  1.096\n",
      "Epoch   1 Batch   21/269, Loss:  1.361\n",
      "Epoch   1 Batch   22/269, Loss:  1.234\n",
      "Epoch   1 Batch   23/269, Loss:  1.430\n",
      "Epoch   1 Batch   24/269, Loss:  1.197\n",
      "Epoch   1 Batch   25/269, Loss:  1.292\n",
      "Epoch   1 Batch   26/269, Loss:  1.170\n",
      "Epoch   1 Batch   27/269, Loss:  1.138\n",
      "Epoch   1 Batch   28/269, Loss:  1.168\n",
      "Epoch   1 Batch   29/269, Loss:  1.071\n",
      "Epoch   1 Batch   30/269, Loss:  1.079\n",
      "Epoch   1 Batch   31/269, Loss:  1.128\n",
      "Epoch   1 Batch   32/269, Loss:  1.088\n",
      "Epoch   1 Batch   33/269, Loss:  1.121\n",
      "Epoch   1 Batch   34/269, Loss:  1.047\n",
      "Epoch   1 Batch   35/269, Loss:  1.220\n",
      "Epoch   1 Batch   36/269, Loss:  1.136\n",
      "Epoch   1 Batch   37/269, Loss:  1.210\n",
      "Epoch   1 Batch   38/269, Loss:  1.162\n",
      "Epoch   1 Batch   39/269, Loss:  1.121\n",
      "Epoch   1 Batch   40/269, Loss:  0.976\n",
      "Epoch   1 Batch   41/269, Loss:  1.073\n",
      "Epoch   1 Batch   42/269, Loss:  1.124\n",
      "Epoch   1 Batch   43/269, Loss:  1.107\n",
      "Epoch   1 Batch   44/269, Loss:  1.075\n",
      "Epoch   1 Batch   45/269, Loss:  1.025\n",
      "Epoch   1 Batch   46/269, Loss:  1.020\n",
      "Epoch   1 Batch   47/269, Loss:  0.924\n",
      "Epoch   1 Batch   48/269, Loss:  0.955\n",
      "Epoch   1 Batch   49/269, Loss:  1.035\n",
      "Epoch   1 Batch   50/269, Loss:  1.090\n",
      "Epoch   1 Batch   51/269, Loss:  1.014\n",
      "Epoch   1 Batch   52/269, Loss:  0.952\n",
      "Epoch   1 Batch   53/269, Loss:  1.056\n",
      "Epoch   1 Batch   54/269, Loss:  0.829\n",
      "Epoch   1 Batch   55/269, Loss:  1.036\n",
      "Epoch   1 Batch   56/269, Loss:  1.073\n",
      "Epoch   1 Batch   57/269, Loss:  1.107\n",
      "Epoch   1 Batch   58/269, Loss:  1.079\n",
      "Epoch   1 Batch   59/269, Loss:  0.963\n",
      "Epoch   1 Batch   60/269, Loss:  0.993\n",
      "Epoch   1 Batch   61/269, Loss:  0.939\n",
      "Epoch   1 Batch   62/269, Loss:  0.990\n",
      "Epoch   1 Batch   63/269, Loss:  1.052\n",
      "Epoch   1 Batch   64/269, Loss:  0.910\n",
      "Epoch   1 Batch   65/269, Loss:  0.899\n",
      "Epoch   1 Batch   66/269, Loss:  0.968\n",
      "Epoch   1 Batch   67/269, Loss:  1.002\n",
      "Epoch   1 Batch   68/269, Loss:  1.222\n",
      "Epoch   1 Batch   69/269, Loss:  1.029\n",
      "Epoch   1 Batch   70/269, Loss:  1.091\n",
      "Epoch   1 Batch   71/269, Loss:  0.980\n",
      "Epoch   1 Batch   72/269, Loss:  1.127\n",
      "Epoch   1 Batch   73/269, Loss:  1.104\n",
      "Epoch   1 Batch   74/269, Loss:  0.895\n",
      "Epoch   1 Batch   75/269, Loss:  0.977\n",
      "Epoch   1 Batch   76/269, Loss:  0.864\n",
      "Epoch   1 Batch   77/269, Loss:  0.892\n",
      "Epoch   1 Batch   78/269, Loss:  0.980\n",
      "Epoch   1 Batch   79/269, Loss:  0.948\n",
      "Epoch   1 Batch   80/269, Loss:  0.873\n",
      "Epoch   1 Batch   81/269, Loss:  0.977\n",
      "Epoch   1 Batch   82/269, Loss:  0.858\n",
      "Epoch   1 Batch   83/269, Loss:  0.887\n",
      "Epoch   1 Batch   84/269, Loss:  0.863\n",
      "Epoch   1 Batch   85/269, Loss:  0.875\n",
      "Epoch   1 Batch   86/269, Loss:  0.910\n",
      "Epoch   1 Batch   87/269, Loss:  0.870\n",
      "Epoch   1 Batch   88/269, Loss:  0.763\n",
      "Epoch   1 Batch   89/269, Loss:  0.937\n",
      "Epoch   1 Batch   90/269, Loss:  0.946\n",
      "Epoch   1 Batch   91/269, Loss:  0.817\n",
      "Epoch   1 Batch   92/269, Loss:  0.827\n",
      "Epoch   1 Batch   93/269, Loss:  0.884\n",
      "Epoch   1 Batch   94/269, Loss:  0.972\n",
      "Epoch   1 Batch   95/269, Loss:  0.824\n",
      "Epoch   1 Batch   96/269, Loss:  0.873\n",
      "Epoch   1 Batch   97/269, Loss:  0.832\n",
      "Epoch   1 Batch   98/269, Loss:  0.745\n",
      "Epoch   1 Batch   99/269, Loss:  0.862\n",
      "Epoch   1 Batch  100/269, Loss:  0.791\n",
      "Epoch   1 Batch  101/269, Loss:  0.858\n",
      "Epoch   1 Batch  102/269, Loss:  0.870\n",
      "Epoch   1 Batch  103/269, Loss:  0.924\n",
      "Epoch   1 Batch  104/269, Loss:  0.786\n",
      "Epoch   1 Batch  105/269, Loss:  0.907\n",
      "Epoch   1 Batch  106/269, Loss:  0.814\n",
      "Epoch   1 Batch  107/269, Loss:  0.884\n",
      "Epoch   1 Batch  108/269, Loss:  0.733\n",
      "Epoch   1 Batch  109/269, Loss:  0.945\n",
      "Epoch   1 Batch  110/269, Loss:  0.779\n",
      "Epoch   1 Batch  111/269, Loss:  0.844\n",
      "Epoch   1 Batch  112/269, Loss:  0.840\n",
      "Epoch   1 Batch  113/269, Loss:  0.907\n",
      "Epoch   1 Batch  114/269, Loss:  0.883\n",
      "Epoch   1 Batch  115/269, Loss:  0.776\n",
      "Epoch   1 Batch  116/269, Loss:  0.771\n",
      "Epoch   1 Batch  117/269, Loss:  0.843\n",
      "Epoch   1 Batch  118/269, Loss:  0.769\n",
      "Epoch   1 Batch  119/269, Loss:  0.834\n",
      "Epoch   1 Batch  120/269, Loss:  0.821\n",
      "Epoch   1 Batch  121/269, Loss:  0.835\n",
      "Epoch   1 Batch  122/269, Loss:  0.832\n",
      "Epoch   1 Batch  123/269, Loss:  0.801\n",
      "Epoch   1 Batch  124/269, Loss:  0.705\n",
      "Epoch   1 Batch  125/269, Loss:  0.864\n",
      "Epoch   1 Batch  126/269, Loss:  0.715\n",
      "Epoch   1 Batch  127/269, Loss:  0.769\n",
      "Epoch   1 Batch  128/269, Loss:  0.769\n",
      "Epoch   1 Batch  129/269, Loss:  0.740\n",
      "Epoch   1 Batch  130/269, Loss:  0.780\n",
      "Epoch   1 Batch  131/269, Loss:  0.815\n",
      "Epoch   1 Batch  132/269, Loss:  0.713\n",
      "Epoch   1 Batch  133/269, Loss:  0.765\n",
      "Epoch   1 Batch  134/269, Loss:  0.775\n",
      "Epoch   1 Batch  135/269, Loss:  0.805\n",
      "Epoch   1 Batch  136/269, Loss:  0.686\n",
      "Epoch   1 Batch  137/269, Loss:  0.705\n",
      "Epoch   1 Batch  138/269, Loss:  0.708\n",
      "Epoch   1 Batch  139/269, Loss:  0.775\n",
      "Epoch   1 Batch  140/269, Loss:  0.712\n",
      "Epoch   1 Batch  141/269, Loss:  0.860\n",
      "Epoch   1 Batch  142/269, Loss:  0.703\n",
      "Epoch   1 Batch  143/269, Loss:  0.718\n",
      "Epoch   1 Batch  144/269, Loss:  0.708\n",
      "Epoch   1 Batch  145/269, Loss:  0.707\n",
      "Epoch   1 Batch  146/269, Loss:  0.675\n",
      "Epoch   1 Batch  147/269, Loss:  0.827\n",
      "Epoch   1 Batch  148/269, Loss:  0.674\n",
      "Epoch   1 Batch  149/269, Loss:  0.694\n",
      "Epoch   1 Batch  150/269, Loss:  0.685\n",
      "Epoch   1 Batch  151/269, Loss:  0.735\n",
      "Epoch   1 Batch  152/269, Loss:  0.697\n",
      "Epoch   1 Batch  153/269, Loss:  0.713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch  154/269, Loss:  0.687\n",
      "Epoch   1 Batch  155/269, Loss:  0.744\n",
      "Epoch   1 Batch  156/269, Loss:  0.706\n",
      "Epoch   1 Batch  157/269, Loss:  0.759\n",
      "Epoch   1 Batch  158/269, Loss:  0.638\n",
      "Epoch   1 Batch  159/269, Loss:  0.690\n",
      "Epoch   1 Batch  160/269, Loss:  0.683\n",
      "Epoch   1 Batch  161/269, Loss:  0.654\n",
      "Epoch   1 Batch  162/269, Loss:  0.682\n",
      "Epoch   1 Batch  163/269, Loss:  0.681\n",
      "Epoch   1 Batch  164/269, Loss:  0.703\n",
      "Epoch   1 Batch  165/269, Loss:  0.769\n",
      "Epoch   1 Batch  166/269, Loss:  0.658\n",
      "Epoch   1 Batch  167/269, Loss:  0.715\n",
      "Epoch   1 Batch  168/269, Loss:  0.708\n",
      "Epoch   1 Batch  169/269, Loss:  0.697\n",
      "Epoch   1 Batch  170/269, Loss:  0.643\n",
      "Epoch   1 Batch  171/269, Loss:  0.657\n",
      "Epoch   1 Batch  172/269, Loss:  0.740\n",
      "Epoch   1 Batch  173/269, Loss:  0.713\n",
      "Epoch   1 Batch  174/269, Loss:  0.634\n",
      "Epoch   1 Batch  175/269, Loss:  0.954\n",
      "Epoch   1 Batch  176/269, Loss:  0.669\n",
      "Epoch   1 Batch  177/269, Loss:  0.662\n",
      "Epoch   1 Batch  178/269, Loss:  0.650\n",
      "Epoch   1 Batch  179/269, Loss:  0.701\n",
      "Epoch   1 Batch  180/269, Loss:  0.634\n",
      "Epoch   1 Batch  181/269, Loss:  0.700\n",
      "Epoch   1 Batch  182/269, Loss:  0.564\n",
      "Epoch   1 Batch  183/269, Loss:  0.663\n",
      "Epoch   1 Batch  184/269, Loss:  0.722\n",
      "Epoch   1 Batch  185/269, Loss:  0.699\n",
      "Epoch   1 Batch  186/269, Loss:  0.681\n",
      "Epoch   1 Batch  187/269, Loss:  0.657\n",
      "Epoch   1 Batch  188/269, Loss:  0.625\n",
      "Epoch   1 Batch  189/269, Loss:  0.741\n",
      "Epoch   1 Batch  190/269, Loss:  0.765\n",
      "Epoch   1 Batch  191/269, Loss:  0.629\n",
      "Epoch   1 Batch  192/269, Loss:  0.654\n",
      "Epoch   1 Batch  193/269, Loss:  0.633\n",
      "Epoch   1 Batch  194/269, Loss:  0.646\n",
      "Epoch   1 Batch  195/269, Loss:  0.748\n",
      "Epoch   1 Batch  196/269, Loss:  0.688\n",
      "Epoch   1 Batch  197/269, Loss:  0.721\n",
      "Epoch   1 Batch  198/269, Loss:  0.707\n",
      "Epoch   1 Batch  199/269, Loss:  0.653\n",
      "Epoch   1 Batch  200/269, Loss:  0.682\n",
      "Epoch   1 Batch  201/269, Loss:  0.650\n",
      "Epoch   1 Batch  202/269, Loss:  0.741\n",
      "Epoch   1 Batch  203/269, Loss:  0.695\n",
      "Epoch   1 Batch  204/269, Loss:  0.703\n",
      "Epoch   1 Batch  205/269, Loss:  0.728\n",
      "Epoch   1 Batch  206/269, Loss:  0.782\n",
      "Epoch   1 Batch  207/269, Loss:  0.658\n",
      "Epoch   1 Batch  208/269, Loss:  0.657\n",
      "Epoch   1 Batch  209/269, Loss:  0.550\n",
      "Epoch   1 Batch  210/269, Loss:  0.664\n",
      "Epoch   1 Batch  211/269, Loss:  0.672\n",
      "Epoch   1 Batch  212/269, Loss:  0.647\n",
      "Epoch   1 Batch  213/269, Loss:  0.712\n",
      "Epoch   1 Batch  214/269, Loss:  0.747\n",
      "Epoch   1 Batch  215/269, Loss:  0.677\n",
      "Epoch   1 Batch  216/269, Loss:  0.758\n",
      "Epoch   1 Batch  217/269, Loss:  0.660\n",
      "Epoch   1 Batch  218/269, Loss:  0.706\n",
      "Epoch   1 Batch  219/269, Loss:  0.658\n",
      "Epoch   1 Batch  220/269, Loss:  0.620\n",
      "Epoch   1 Batch  221/269, Loss:  0.582\n",
      "Epoch   1 Batch  222/269, Loss:  0.603\n",
      "Epoch   1 Batch  223/269, Loss:  0.607\n",
      "Epoch   1 Batch  224/269, Loss:  0.648\n",
      "Epoch   1 Batch  225/269, Loss:  0.646\n",
      "Epoch   1 Batch  226/269, Loss:  0.670\n",
      "Epoch   1 Batch  227/269, Loss:  0.687\n",
      "Epoch   1 Batch  228/269, Loss:  0.553\n",
      "Epoch   1 Batch  229/269, Loss:  0.595\n",
      "Epoch   1 Batch  230/269, Loss:  0.703\n",
      "Epoch   1 Batch  231/269, Loss:  0.668\n",
      "Epoch   1 Batch  232/269, Loss:  0.659\n",
      "Epoch   1 Batch  233/269, Loss:  0.623\n",
      "Epoch   1 Batch  234/269, Loss:  0.665\n",
      "Epoch   1 Batch  235/269, Loss:  0.700\n",
      "Epoch   1 Batch  236/269, Loss:  0.720\n",
      "Epoch   1 Batch  237/269, Loss:  0.641\n",
      "Epoch   1 Batch  238/269, Loss:  0.643\n",
      "Epoch   1 Batch  239/269, Loss:  0.602\n",
      "Epoch   1 Batch  240/269, Loss:  0.553\n",
      "Epoch   1 Batch  241/269, Loss:  0.732\n",
      "Epoch   1 Batch  242/269, Loss:  0.600\n",
      "Epoch   1 Batch  243/269, Loss:  0.554\n",
      "Epoch   1 Batch  244/269, Loss:  0.655\n",
      "Epoch   1 Batch  245/269, Loss:  0.628\n",
      "Epoch   1 Batch  246/269, Loss:  0.760\n",
      "Epoch   1 Batch  247/269, Loss:  0.632\n",
      "Epoch   1 Batch  248/269, Loss:  0.630\n",
      "Epoch   1 Batch  249/269, Loss:  0.623\n",
      "Epoch   1 Batch  250/269, Loss:  0.619\n",
      "Epoch   1 Batch  251/269, Loss:  0.603\n",
      "Epoch   1 Batch  252/269, Loss:  0.566\n",
      "Epoch   1 Batch  253/269, Loss:  0.639\n",
      "Epoch   1 Batch  254/269, Loss:  0.625\n",
      "Epoch   1 Batch  255/269, Loss:  0.612\n",
      "Epoch   1 Batch  256/269, Loss:  0.689\n",
      "Epoch   1 Batch  257/269, Loss:  0.713\n",
      "Epoch   1 Batch  258/269, Loss:  0.634\n",
      "Epoch   1 Batch  259/269, Loss:  0.639\n",
      "Epoch   1 Batch  260/269, Loss:  0.555\n",
      "Epoch   1 Batch  261/269, Loss:  0.592\n",
      "Epoch   1 Batch  262/269, Loss:  0.675\n",
      "Epoch   1 Batch  263/269, Loss:  0.647\n",
      "Epoch   1 Batch  264/269, Loss:  0.568\n",
      "Epoch   1 Batch  265/269, Loss:  0.614\n",
      "Epoch   1 Batch  266/269, Loss:  0.619\n",
      "Epoch   1 Batch  267/269, Loss:  0.578\n",
      "Epoch   2 Batch    0/269, Loss:  0.649\n",
      "Epoch   2 Batch    1/269, Loss:  0.560\n",
      "Epoch   2 Batch    2/269, Loss:  0.648\n",
      "Epoch   2 Batch    3/269, Loss:  0.632\n",
      "Epoch   2 Batch    4/269, Loss:  0.606\n",
      "Epoch   2 Batch    5/269, Loss:  0.578\n",
      "Epoch   2 Batch    6/269, Loss:  0.661\n",
      "Epoch   2 Batch    7/269, Loss:  0.626\n",
      "Epoch   2 Batch    8/269, Loss:  0.616\n",
      "Epoch   2 Batch    9/269, Loss:  0.577\n",
      "Epoch   2 Batch   10/269, Loss:  0.514\n",
      "Epoch   2 Batch   11/269, Loss:  0.609\n",
      "Epoch   2 Batch   12/269, Loss:  0.594\n",
      "Epoch   2 Batch   13/269, Loss:  0.630\n",
      "Epoch   2 Batch   14/269, Loss:  0.670\n",
      "Epoch   2 Batch   15/269, Loss:  0.575\n",
      "Epoch   2 Batch   16/269, Loss:  0.556\n",
      "Epoch   2 Batch   17/269, Loss:  0.553\n",
      "Epoch   2 Batch   18/269, Loss:  0.561\n",
      "Epoch   2 Batch   19/269, Loss:  0.579\n",
      "Epoch   2 Batch   20/269, Loss:  0.570\n",
      "Epoch   2 Batch   21/269, Loss:  0.638\n",
      "Epoch   2 Batch   22/269, Loss:  0.569\n",
      "Epoch   2 Batch   23/269, Loss:  0.625\n",
      "Epoch   2 Batch   24/269, Loss:  0.589\n",
      "Epoch   2 Batch   25/269, Loss:  0.605\n",
      "Epoch   2 Batch   26/269, Loss:  0.575\n",
      "Epoch   2 Batch   27/269, Loss:  0.615\n",
      "Epoch   2 Batch   28/269, Loss:  0.653\n",
      "Epoch   2 Batch   29/269, Loss:  0.584\n",
      "Epoch   2 Batch   30/269, Loss:  0.588\n",
      "Epoch   2 Batch   31/269, Loss:  0.613\n",
      "Epoch   2 Batch   32/269, Loss:  0.639\n",
      "Epoch   2 Batch   33/269, Loss:  0.624\n",
      "Epoch   2 Batch   34/269, Loss:  0.546\n",
      "Epoch   2 Batch   35/269, Loss:  0.733\n",
      "Epoch   2 Batch   36/269, Loss:  0.620\n",
      "Epoch   2 Batch   37/269, Loss:  0.615\n",
      "Epoch   2 Batch   38/269, Loss:  0.625\n",
      "Epoch   2 Batch   39/269, Loss:  0.602\n",
      "Epoch   2 Batch   40/269, Loss:  0.597\n",
      "Epoch   2 Batch   41/269, Loss:  0.599\n",
      "Epoch   2 Batch   42/269, Loss:  0.567\n",
      "Epoch   2 Batch   43/269, Loss:  0.621\n",
      "Epoch   2 Batch   44/269, Loss:  0.562\n",
      "Epoch   2 Batch   45/269, Loss:  0.592\n",
      "Epoch   2 Batch   46/269, Loss:  0.617\n",
      "Epoch   2 Batch   47/269, Loss:  0.576\n",
      "Epoch   2 Batch   48/269, Loss:  0.633\n",
      "Epoch   2 Batch   49/269, Loss:  0.616\n",
      "Epoch   2 Batch   50/269, Loss:  0.538\n",
      "Epoch   2 Batch   51/269, Loss:  0.559\n",
      "Epoch   2 Batch   52/269, Loss:  0.571\n",
      "Epoch   2 Batch   53/269, Loss:  0.600\n",
      "Epoch   2 Batch   54/269, Loss:  0.487\n",
      "Epoch   2 Batch   55/269, Loss:  0.578\n",
      "Epoch   2 Batch   56/269, Loss:  0.578\n",
      "Epoch   2 Batch   57/269, Loss:  0.598\n",
      "Epoch   2 Batch   58/269, Loss:  0.687\n",
      "Epoch   2 Batch   59/269, Loss:  0.575\n",
      "Epoch   2 Batch   60/269, Loss:  0.539\n",
      "Epoch   2 Batch   61/269, Loss:  0.592\n",
      "Epoch   2 Batch   62/269, Loss:  0.540\n",
      "Epoch   2 Batch   63/269, Loss:  0.523\n",
      "Epoch   2 Batch   64/269, Loss:  0.596\n",
      "Epoch   2 Batch   65/269, Loss:  0.597\n",
      "Epoch   2 Batch   66/269, Loss:  0.587\n",
      "Epoch   2 Batch   67/269, Loss:  0.591\n",
      "Epoch   2 Batch   68/269, Loss:  0.793\n",
      "Epoch   2 Batch   69/269, Loss:  0.596\n",
      "Epoch   2 Batch   70/269, Loss:  0.642\n",
      "Epoch   2 Batch   71/269, Loss:  0.570\n",
      "Epoch   2 Batch   72/269, Loss:  0.618\n",
      "Epoch   2 Batch   73/269, Loss:  0.605\n",
      "Epoch   2 Batch   74/269, Loss:  0.568\n",
      "Epoch   2 Batch   75/269, Loss:  0.721\n",
      "Epoch   2 Batch   76/269, Loss:  0.596\n",
      "Epoch   2 Batch   77/269, Loss:  0.545\n",
      "Epoch   2 Batch   78/269, Loss:  0.669\n",
      "Epoch   2 Batch   79/269, Loss:  0.598\n",
      "Epoch   2 Batch   80/269, Loss:  0.628\n",
      "Epoch   2 Batch   81/269, Loss:  0.533\n",
      "Epoch   2 Batch   82/269, Loss:  0.506\n",
      "Epoch   2 Batch   83/269, Loss:  0.643\n",
      "Epoch   2 Batch   84/269, Loss:  0.607\n",
      "Epoch   2 Batch   85/269, Loss:  0.502\n",
      "Epoch   2 Batch   86/269, Loss:  0.544\n",
      "Epoch   2 Batch   87/269, Loss:  0.580\n",
      "Epoch   2 Batch   88/269, Loss:  0.513\n",
      "Epoch   2 Batch   89/269, Loss:  0.659\n",
      "Epoch   2 Batch   90/269, Loss:  0.674\n",
      "Epoch   2 Batch   91/269, Loss:  0.607\n",
      "Epoch   2 Batch   92/269, Loss:  0.629\n",
      "Epoch   2 Batch   93/269, Loss:  0.571\n",
      "Epoch   2 Batch   94/269, Loss:  0.643\n",
      "Epoch   2 Batch   95/269, Loss:  0.589\n",
      "Epoch   2 Batch   96/269, Loss:  0.633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 Batch   97/269, Loss:  0.605\n",
      "Epoch   2 Batch   98/269, Loss:  0.541\n",
      "Epoch   2 Batch   99/269, Loss:  0.610\n",
      "Epoch   2 Batch  100/269, Loss:  0.553\n",
      "Epoch   2 Batch  101/269, Loss:  0.582\n",
      "Epoch   2 Batch  102/269, Loss:  0.605\n",
      "Epoch   2 Batch  103/269, Loss:  0.597\n",
      "Epoch   2 Batch  104/269, Loss:  0.590\n",
      "Epoch   2 Batch  105/269, Loss:  0.575\n",
      "Epoch   2 Batch  106/269, Loss:  0.515\n",
      "Epoch   2 Batch  107/269, Loss:  0.591\n",
      "Epoch   2 Batch  108/269, Loss:  0.501\n",
      "Epoch   2 Batch  109/269, Loss:  0.633\n",
      "Epoch   2 Batch  110/269, Loss:  0.532\n",
      "Epoch   2 Batch  111/269, Loss:  0.603\n",
      "Epoch   2 Batch  112/269, Loss:  0.628\n",
      "Epoch   2 Batch  113/269, Loss:  0.578\n",
      "Epoch   2 Batch  114/269, Loss:  0.566\n",
      "Epoch   2 Batch  115/269, Loss:  0.589\n",
      "Epoch   2 Batch  116/269, Loss:  0.602\n",
      "Epoch   2 Batch  117/269, Loss:  0.585\n",
      "Epoch   2 Batch  118/269, Loss:  0.520\n",
      "Epoch   2 Batch  119/269, Loss:  0.558\n",
      "Epoch   2 Batch  120/269, Loss:  0.588\n",
      "Epoch   2 Batch  121/269, Loss:  0.555\n",
      "Epoch   2 Batch  122/269, Loss:  0.603\n",
      "Epoch   2 Batch  123/269, Loss:  0.597\n",
      "Epoch   2 Batch  124/269, Loss:  0.533\n",
      "Epoch   2 Batch  125/269, Loss:  0.638\n",
      "Epoch   2 Batch  126/269, Loss:  0.501\n",
      "Epoch   2 Batch  127/269, Loss:  0.582\n",
      "Epoch   2 Batch  128/269, Loss:  0.586\n",
      "Epoch   2 Batch  129/269, Loss:  0.551\n",
      "Epoch   2 Batch  130/269, Loss:  0.583\n",
      "Epoch   2 Batch  131/269, Loss:  0.576\n",
      "Epoch   2 Batch  132/269, Loss:  0.568\n",
      "Epoch   2 Batch  133/269, Loss:  0.548\n",
      "Epoch   2 Batch  134/269, Loss:  0.575\n",
      "Epoch   2 Batch  135/269, Loss:  0.609\n",
      "Epoch   2 Batch  136/269, Loss:  0.500\n",
      "Epoch   2 Batch  137/269, Loss:  0.519\n",
      "Epoch   2 Batch  138/269, Loss:  0.590\n",
      "Epoch   2 Batch  139/269, Loss:  0.547\n",
      "Epoch   2 Batch  140/269, Loss:  0.530\n",
      "Epoch   2 Batch  141/269, Loss:  0.620\n",
      "Epoch   2 Batch  142/269, Loss:  0.562\n",
      "Epoch   2 Batch  143/269, Loss:  0.496\n",
      "Epoch   2 Batch  144/269, Loss:  0.553\n",
      "Epoch   2 Batch  145/269, Loss:  0.475\n",
      "Epoch   2 Batch  146/269, Loss:  0.570\n",
      "Epoch   2 Batch  147/269, Loss:  0.583\n",
      "Epoch   2 Batch  148/269, Loss:  0.506\n",
      "Epoch   2 Batch  149/269, Loss:  0.511\n",
      "Epoch   2 Batch  150/269, Loss:  0.561\n",
      "Epoch   2 Batch  151/269, Loss:  0.530\n",
      "Epoch   2 Batch  152/269, Loss:  0.514\n",
      "Epoch   2 Batch  153/269, Loss:  0.586\n",
      "Epoch   2 Batch  154/269, Loss:  0.521\n",
      "Epoch   2 Batch  155/269, Loss:  0.530\n",
      "Epoch   2 Batch  156/269, Loss:  0.523\n",
      "Epoch   2 Batch  157/269, Loss:  0.548\n",
      "Epoch   2 Batch  158/269, Loss:  0.478\n",
      "Epoch   2 Batch  159/269, Loss:  0.556\n",
      "Epoch   2 Batch  160/269, Loss:  0.533\n",
      "Epoch   2 Batch  161/269, Loss:  0.510\n",
      "Epoch   2 Batch  162/269, Loss:  0.527\n",
      "Epoch   2 Batch  163/269, Loss:  0.530\n",
      "Epoch   2 Batch  164/269, Loss:  0.541\n",
      "Epoch   2 Batch  165/269, Loss:  0.614\n",
      "Epoch   2 Batch  166/269, Loss:  0.534\n",
      "Epoch   2 Batch  167/269, Loss:  0.532\n",
      "Epoch   2 Batch  168/269, Loss:  0.542\n",
      "Epoch   2 Batch  169/269, Loss:  0.551\n",
      "Epoch   2 Batch  170/269, Loss:  0.542\n",
      "Epoch   2 Batch  171/269, Loss:  0.571\n",
      "Epoch   2 Batch  172/269, Loss:  0.554\n",
      "Epoch   2 Batch  173/269, Loss:  0.533\n",
      "Epoch   2 Batch  174/269, Loss:  0.601\n",
      "Epoch   2 Batch  175/269, Loss:  0.681\n",
      "Epoch   2 Batch  176/269, Loss:  0.536\n",
      "Epoch   2 Batch  177/269, Loss:  0.481\n",
      "Epoch   2 Batch  178/269, Loss:  0.554\n",
      "Epoch   2 Batch  179/269, Loss:  0.536\n",
      "Epoch   2 Batch  180/269, Loss:  0.486\n",
      "Epoch   2 Batch  181/269, Loss:  0.542\n",
      "Epoch   2 Batch  182/269, Loss:  0.497\n",
      "Epoch   2 Batch  183/269, Loss:  0.540\n",
      "Epoch   2 Batch  184/269, Loss:  0.603\n",
      "Epoch   2 Batch  185/269, Loss:  0.560\n",
      "Epoch   2 Batch  186/269, Loss:  0.558\n",
      "Epoch   2 Batch  187/269, Loss:  0.571\n",
      "Epoch   2 Batch  188/269, Loss:  0.451\n",
      "Epoch   2 Batch  189/269, Loss:  0.549\n",
      "Epoch   2 Batch  190/269, Loss:  0.552\n",
      "Epoch   2 Batch  191/269, Loss:  0.494\n",
      "Epoch   2 Batch  192/269, Loss:  0.538\n",
      "Epoch   2 Batch  193/269, Loss:  0.540\n",
      "Epoch   2 Batch  194/269, Loss:  0.529\n",
      "Epoch   2 Batch  195/269, Loss:  0.583\n",
      "Epoch   2 Batch  196/269, Loss:  0.526\n",
      "Epoch   2 Batch  197/269, Loss:  0.530\n",
      "Epoch   2 Batch  198/269, Loss:  0.523\n",
      "Epoch   2 Batch  199/269, Loss:  0.584\n",
      "Epoch   2 Batch  200/269, Loss:  0.559\n",
      "Epoch   2 Batch  201/269, Loss:  0.621\n",
      "Epoch   2 Batch  202/269, Loss:  0.570\n",
      "Epoch   2 Batch  203/269, Loss:  0.544\n",
      "Epoch   2 Batch  204/269, Loss:  0.550\n",
      "Epoch   2 Batch  205/269, Loss:  0.555\n",
      "Epoch   2 Batch  206/269, Loss:  0.591\n",
      "Epoch   2 Batch  207/269, Loss:  0.517\n",
      "Epoch   2 Batch  208/269, Loss:  0.582\n",
      "Epoch   2 Batch  209/269, Loss:  0.466\n",
      "Epoch   2 Batch  210/269, Loss:  0.514\n",
      "Epoch   2 Batch  211/269, Loss:  0.525\n",
      "Epoch   2 Batch  212/269, Loss:  0.523\n",
      "Epoch   2 Batch  213/269, Loss:  0.559\n",
      "Epoch   2 Batch  214/269, Loss:  0.557\n",
      "Epoch   2 Batch  215/269, Loss:  0.516\n",
      "Epoch   2 Batch  216/269, Loss:  0.567\n",
      "Epoch   2 Batch  217/269, Loss:  0.564\n",
      "Epoch   2 Batch  218/269, Loss:  0.634\n",
      "Epoch   2 Batch  219/269, Loss:  0.556\n",
      "Epoch   2 Batch  220/269, Loss:  0.516\n",
      "Epoch   2 Batch  221/269, Loss:  0.520\n",
      "Epoch   2 Batch  222/269, Loss:  0.500\n",
      "Epoch   2 Batch  223/269, Loss:  0.517\n",
      "Epoch   2 Batch  224/269, Loss:  0.547\n",
      "Epoch   2 Batch  225/269, Loss:  0.583\n",
      "Epoch   2 Batch  226/269, Loss:  0.535\n",
      "Epoch   2 Batch  227/269, Loss:  0.593\n",
      "Epoch   2 Batch  228/269, Loss:  0.505\n",
      "Epoch   2 Batch  229/269, Loss:  0.476\n",
      "Epoch   2 Batch  230/269, Loss:  0.573\n",
      "Epoch   2 Batch  231/269, Loss:  0.505\n",
      "Epoch   2 Batch  232/269, Loss:  0.587\n",
      "Epoch   2 Batch  233/269, Loss:  0.504\n",
      "Epoch   2 Batch  234/269, Loss:  0.566\n",
      "Epoch   2 Batch  235/269, Loss:  0.546\n",
      "Epoch   2 Batch  236/269, Loss:  0.488\n",
      "Epoch   2 Batch  237/269, Loss:  0.492\n",
      "Epoch   2 Batch  238/269, Loss:  0.524\n",
      "Epoch   2 Batch  239/269, Loss:  0.528\n",
      "Epoch   2 Batch  240/269, Loss:  0.517\n",
      "Epoch   2 Batch  241/269, Loss:  0.545\n",
      "Epoch   2 Batch  242/269, Loss:  0.499\n",
      "Epoch   2 Batch  243/269, Loss:  0.489\n",
      "Epoch   2 Batch  244/269, Loss:  0.495\n",
      "Epoch   2 Batch  245/269, Loss:  0.508\n",
      "Epoch   2 Batch  246/269, Loss:  0.620\n",
      "Epoch   2 Batch  247/269, Loss:  0.529\n",
      "Epoch   2 Batch  248/269, Loss:  0.600\n",
      "Epoch   2 Batch  249/269, Loss:  0.527\n",
      "Epoch   2 Batch  250/269, Loss:  0.474\n",
      "Epoch   2 Batch  251/269, Loss:  0.521\n",
      "Epoch   2 Batch  252/269, Loss:  0.520\n",
      "Epoch   2 Batch  253/269, Loss:  0.553\n",
      "Epoch   2 Batch  254/269, Loss:  0.530\n",
      "Epoch   2 Batch  255/269, Loss:  0.485\n",
      "Epoch   2 Batch  256/269, Loss:  0.563\n",
      "Epoch   2 Batch  257/269, Loss:  0.538\n",
      "Epoch   2 Batch  258/269, Loss:  0.511\n",
      "Epoch   2 Batch  259/269, Loss:  0.516\n",
      "Epoch   2 Batch  260/269, Loss:  0.506\n",
      "Epoch   2 Batch  261/269, Loss:  0.489\n",
      "Epoch   2 Batch  262/269, Loss:  0.551\n",
      "Epoch   2 Batch  263/269, Loss:  0.570\n",
      "Epoch   2 Batch  264/269, Loss:  0.506\n",
      "Epoch   2 Batch  265/269, Loss:  0.510\n",
      "Epoch   2 Batch  266/269, Loss:  0.508\n",
      "Epoch   2 Batch  267/269, Loss:  0.497\n",
      "Training time:  0.2767636775970459\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    lower_case_words = [word.lower() for word in sentence.split()]\n",
    "    \n",
    "    word_id = [vocab_to_int.get(word, vocab_to_int['<unk>']) for word in lower_case_words]\n",
    "    \n",
    "    return word_id\n",
    "\n",
    "import time\n",
    "\n",
    "### TRAINING ###\n",
    "save_path = 'checkpoints/dev'\n",
    "\n",
    "train_source = source_int_text[train_hparams.batch_size:]\n",
    "train_target = target_int_text[train_hparams.batch_size:]\n",
    "\n",
    "valid_source = source_int_text[:train_hparams.batch_size]\n",
    "valid_target = target_int_text[:train_hparams.batch_size]\n",
    "\n",
    "get_accuracy_every = 30\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    steps = 0\n",
    "    \n",
    "    for epoch_i in range(train_hparams.epochs):\n",
    "        \n",
    "        step = 0\n",
    "        for batch_i, (source_batch, target_batch) in enumerate(\n",
    "                batch_data(train_source, train_target, train_hparams.batch_size)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            source_batch_seq_lenght = []\n",
    "            for item in source_batch:\n",
    "                source_batch_seq_lenght.append(np.shape(item)[0])\n",
    "            \n",
    "            target_batch_seq_lenght = []\n",
    "            for item in target_batch:\n",
    "                target_batch_seq_lenght.append(np.shape(item)[0])\n",
    "                \n",
    "#             if (source_batch_seq_lenght[0] > 300): #OOM problems with datasets containing very large sentences\n",
    "#                 continue\n",
    "                \n",
    "            _, loss_val = sess.run(\n",
    "                [updates, loss],\n",
    "                {encoder_inputs: source_batch,\n",
    "                 decoder_inputs: target_batch,\n",
    "                encoder_inputs_length: source_batch_seq_lenght,\n",
    "                decoder_inputs_length: target_batch_seq_lenght})\n",
    "\n",
    "            print('Epoch {:>3} Batch {:>4}/{}, Loss: {:>6.3f}'\n",
    "                  .format(epoch_i, batch_i, len(source_int_text) // train_hparams.batch_size, loss_val))\n",
    "                \n",
    "            end_time = time.time()\n",
    "            \n",
    "    print(\"Training time: \", end_time - start_time)\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "save_params(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "(16,)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [273, 88, 69, 78, 37, 151, 6, 124, 49, 239, 39, 69, 140, 31, 313, 215]\n",
      "  Source Words: ['new', 'jersey', 'est', 'parfois', 'calme', 'pendant', \"l'\", 'automne', ',', 'et', 'il', 'est', 'neigeux', 'en', 'avril', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [135, 135, 19, 110, 110, 98, 130, 178, 31, 31, 193, 31, 115, 160, 225, 225, 8, 225, 121, 225, 8, 160, 225, 8, 160, 225, 8, 160, 225, 8, 225, 29]\n",
      "  Predicted Words: ['been', 'been', 'little', 'fruit', 'fruit', 'my', 'lemons', 'cats', 'rabbit', 'rabbit', 'snakes', 'rabbit', '.', 'never', 'summer', 'summer', 'last', 'summer', ',', 'summer', 'last', 'never', 'summer', 'last', 'never', 'summer', 'last', 'never', 'summer', 'last', 'summer', 'tower']\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = \"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"\n",
    "#fr to en\n",
    "#input: \"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"\n",
    "#target:\"new jersey is sometimes quiet during autumn , and it is snowy in april .\"\n",
    "\n",
    "#en to vi\n",
    "#input:  \"H vit gn 1000 trang v ch  ny .\"\n",
    "#target: \"They wrote almost a thousand pages on the topic .\"\n",
    "\n",
    "print(translate_sentence)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "print(np.shape(translate_sentence))\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    encoder_inputs = loaded_graph.get_tensor_by_name('encoder_inputs:0')\n",
    "    encoder_inputs_length = loaded_graph.get_tensor_by_name('encoder_inputs_length:0')\n",
    "    decoder_pred_decode = loaded_graph.get_tensor_by_name('decoder_pred_decode:0')\n",
    "    \n",
    "    predicted_ids = sess.run(decoder_pred_decode, {encoder_inputs: [translate_sentence],\n",
    "                                                       encoder_inputs_length: [np.shape(translate_sentence)[0]]})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  Source Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i[0] for i in predicted_ids]))\n",
    "print('  Predicted Words: {}'.format([target_int_to_vocab[i[0]] for i in predicted_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translation:\n",
      "\n",
      "been been little fruit fruit my lemons cats rabbit rabbit snakes rabbit . never summer summer last summer , summer last never summer last never summer last never summer last summer tower \n"
     ]
    }
   ],
   "source": [
    "print('\\nTranslation:\\n')\n",
    "translation = ''\n",
    "for word_i in predicted_ids:\n",
    "    translation += target_int_to_vocab[word_i[0]] + ' '\n",
    "    \n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
